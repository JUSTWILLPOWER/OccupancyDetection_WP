{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pyqtgraph as pg\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "%gui qt5\n",
    "import KMedoids\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plottig GUI\n",
    "app = QtGui.QApplication([])\n",
    "pg.setConfigOption('background','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsWindow(title=\"Occupancy Detection GUI\")\n",
    "plot1 = win.addPlot()\n",
    "plot1.setXRange(-6,6)\n",
    "plot1.setYRange(0,6)\n",
    "plot1.setLabel('left',text = 'Y position (m)')\n",
    "plot1.setLabel('bottom', text= 'X position (m)')\n",
    "s1 = plot1.plot([],[],pen=None,symbol='o')\n",
    "plot2 = win.addPlot()\n",
    "plot2.setXRange(-6,6)\n",
    "plot2.setYRange(0,6)\n",
    "plot2.setLabel('left',text = 'Y position (m)')\n",
    "plot2.setLabel('bottom', text= 'X position (m)')\n",
    "s2 = plot2.plot([],[],pen=None,symbol='o')\n",
    "plot3 = win.addPlot()\n",
    "plot3.setXRange(-6,6)\n",
    "plot3.setYRange(0,6)\n",
    "plot3.setLabel('left',text = 'Y position (m)')\n",
    "plot3.setLabel('bottom', text= 'X position (m)')\n",
    "s3 = plot3.plot([],[],pen=None,symbol='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlvParsing(data, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes):\n",
    "    \n",
    "    data = np.frombuffer(data, dtype = 'uint8')\n",
    "    \n",
    "    targetDict = dict()\n",
    "    pointCloud = np.array([])\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index+4:index+8].view(dtype=np.uint32)\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    pointCloudDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType.size > 0 and tlvType == 6: #point cloud TLV\n",
    "        numberOfPoints = pointCloudDataLength/pointLengthInBytes\n",
    "#         print('NUMBER OF POINTS ', str(int(numberOfPoints)))\n",
    "        if numberOfPoints > 0:\n",
    "            p = data[index:index+pointCloudDataLength[0]].view(dtype=np.single)\n",
    "            #form the appropriate array \n",
    "            #each point is 16 bytes - 4 bytes for each property - range, azimuth, doppler, snr\n",
    "            pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "    \n",
    "    #increment the index so it is possible to read the target list\n",
    "    index += pointCloudDataLength\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index[0]:index[0]+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index[0]+4:index[0]+8].view(dtype=np.uint32)\n",
    "    index += tlvHeaderLengthInBytes\n",
    "    targetListDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType.size > 0 and tlvType == 7: #target List TLV\n",
    "        \n",
    "        numberOfTargets = targetListDataLength/targetLengthInBytes\n",
    "        TID = np.zeros((1, int(numberOfTargets[0])), dtype = np.uint32) #tracking IDs\n",
    "        kinematicData = np.zeros((6, int(numberOfTargets[0])), dtype = np.single)\n",
    "        errorCovariance = np.zeros((9, int(numberOfTargets[0])), dtype = np.single)\n",
    "        gatingGain = np.zeros((1, int(numberOfTargets[0])), dtype = np.single)\n",
    "        \n",
    "        #increment the index so it is possible to read the target list\n",
    "        targetIndex = 0\n",
    "        while targetIndex != int(numberOfTargets[0]):\n",
    "            TID[0][targetIndex] = data[index[0]:index[0]+4].view(dtype=np.uint32)\n",
    "            kinematicData[:,targetIndex] = data[index[0]+4:index[0]+28].view(dtype=np.single)\n",
    "            errorCovariance[:,targetIndex] = data[index[0]+28:index[0]+64].view(dtype=np.single)\n",
    "            gatingGain[:,targetIndex] = data[index[0]+64:index[0]+68].view(dtype=np.single)\n",
    "            index += targetLengthInBytes\n",
    "            targetIndex += 1\n",
    "            \n",
    "        targetDict['TID'] = TID\n",
    "        targetDict['kinematicData'] = kinematicData\n",
    "        targetDict['errorCovariance'] = errorCovariance\n",
    "        targetDict['gatingGain'] = gatingGain\n",
    "    \n",
    "    return pointCloud, targetDict\n",
    "\n",
    "def parsePointCloud(pointCloud): #remove points that are not within the boundary\n",
    "    \n",
    "    effectivePointCloud = np.array([])\n",
    "    \n",
    "    for index in range(0, len(pointCloud[0,:])):\n",
    "        if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "        and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "            and pointCloud[1, index] < 50*np.pi/180):\n",
    "\n",
    "            #concatenate columns to the new point cloud\n",
    "            if len(effectivePointCloud) == 0:\n",
    "                effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "            else:\n",
    "                point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "\n",
    "    if len(effectivePointCloud) != 0:\n",
    "        posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "        posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "        SNR  = effectivePointCloud[3,:]\n",
    "    \n",
    "        return posX,posY,SNR\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterativeDfs(vertexID, edgeMatrix, startNode):\n",
    "    \n",
    "    visited = np.array([], dtype=np.int)\n",
    "    dfsStack = np.array([startNode])\n",
    "\n",
    "    while dfsStack.size > 0:\n",
    "        vertex, dfsStack = dfsStack[-1], dfsStack[:-1] #equivalent to stack pop function\n",
    "        if vertex not in visited:\n",
    "            #find unvisited nodes\n",
    "            unvisitedNodes = vertexID[np.logical_not(np.isnan(edgeMatrix[int(vertex), :]))]\n",
    "            visited = np.append(visited, vertex)\n",
    "            #add unvisited nodes to the stack\n",
    "            dfsStack = np.append(dfsStack, unvisitedNodes[np.logical_not(np.isin(unvisitedNodes,visited))])\n",
    "    \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreeClusteringOnly(posX, posY, SNR, weightThreshold, minClusterSize):\n",
    "    \n",
    "    vertexID = np.arange(len(posX))\n",
    "    vertexList = np.arange(len(posX))\n",
    "\n",
    "    associatedPoints = np.array([])\n",
    "\n",
    "    if len(posX) >= minClusterSize:\n",
    "        edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "\n",
    "        #create distance matrix\n",
    "        #x1 - x0\n",
    "        xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                                  np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "        #y1 - y0\n",
    "        yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                                  np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "        #euclidean distance calculation\n",
    "        edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "        #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "        weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "        edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "        #perform iterative dfs\n",
    "        associatedPoints = np.array([])\n",
    "        \n",
    "        \n",
    "        centroidNumber = 0\n",
    "        while vertexID.size > 0:\n",
    "            startNode = vertexID[0]\n",
    "            visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "            #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "            vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "#             #visited is a component, extract cluster from it if possible\n",
    "            if visited.size >= minClusterSize:\n",
    "                cluster =  np.array([posX[visited], posY[visited],SNR[visited],\n",
    "                                     np.repeat(centroidNumber, repeats=len(visited))])\n",
    "                if associatedPoints.size == 0:\n",
    "                    associatedPoints = cluster\n",
    "                else:\n",
    "                    associatedPoints = np.hstack((associatedPoints, cluster))\n",
    "                centroidNumber += 1\n",
    "                                    \n",
    "                \n",
    "\n",
    "\n",
    "    return associatedPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DKF(x, P, R, Q=0., dt=1.0):\n",
    "    \"\"\" Returns a KalmanFilter which implements a\n",
    "    constant velocity model for a state [x dx].T\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KalmanFilter(dim_x=4, dim_z=2)\n",
    "    kf.x = np.array([x[0], x[1]]) # location and velocity\n",
    "    kf.F = np.array([[1., dt],\n",
    "                     [0.,  1.]])  # state transition matrix\n",
    "    kf.H = np.array([[1., 0]])    # Measurement function\n",
    "    kf.R *= R                     # measurement uncertainty\n",
    "    if np.isscalar(P):\n",
    "        kf.P *= P                 # covariance matrix \n",
    "    else:\n",
    "        kf.P[:] = P               # [:] makes deep copy\n",
    "    if np.isscalar(Q):\n",
    "        kf.Q = Q_discrete_white_noise(dim=2, dt=dt, var=Q)\n",
    "    else:\n",
    "        kf.Q[:] = Q\n",
    "    return kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsingMatFile = 'C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Experiment Data 2\\\\3PeopleWalking.mat'\n",
    "\n",
    "tlvData = (loadmat(parsingMatFile))['tlvStream'][0]\n",
    "\n",
    "#RKF \n",
    "centroidX =np.zeros((4,1))\n",
    "centroidP = []\n",
    "P = np.identity(4);\n",
    "centroidP.extend([P])\n",
    "\n",
    "#DKF\n",
    "#initialize variables\n",
    "deltaT = 50*10**-3 #50ms\n",
    "\n",
    "#system matrix\n",
    "A = np.array([\n",
    "    [1, deltaT, 0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,deltaT],\n",
    "    [0,0,0,1]\n",
    "])\n",
    "\n",
    "#output matrix\n",
    "H = np.array([[1,0,0,0],\n",
    "              [0,0,1,0]])\n",
    "#covariance matrices\n",
    "Q = np.eye(4)\n",
    "R = np.ones(2).reshape(-1,1)\n",
    "Pc = np.eye(4)\n",
    "\n",
    "Pd = np.add(np.matmul(np.matmul(A,Pc), np.transpose(A)), Q) #prediction covariance\n",
    "S = np.add(R, np.matmul(np.matmul(H, Pd), np.transpose(H))) #innovation covariance\n",
    "K = np.matmul(Pd, np.matmul(np.transpose(H), np.linalg.inv(S))) #kalman gain\n",
    "Pupdate = np.subtract(Pd, np.matmul(np.matmul(K, S), np.transpose(K))) #update covariance\n",
    "\n",
    "\n",
    "#tree based\n",
    "weightThresholdIntial = 0.2 #minimum distance between points\n",
    "minClusterSizeInitial = 10\n",
    "weightThresholdFinal = 0.8 #minimum distance between points\n",
    "minClusterSizeFinal = 8 \n",
    "\n",
    "\n",
    "#zone snr\n",
    "snrFirstZone = 10\n",
    "snrMiddleZone = 15\n",
    "snrLastZone = 5\n",
    "\n",
    "tlvHeaderLengthInBytes = 8\n",
    "pointLengthInBytes = 16\n",
    "targetLengthInBytes = 68\n",
    "\n",
    "#optimization\n",
    "expectedClustering = np.array([])\n",
    "expectedKalman = np.array([])\n",
    "\n",
    "s1.setData([],[])\n",
    "s2.setData([],[])\n",
    "s3.setData([],[])\n",
    "s4.setData([],[])\n",
    "QtGui.QApplication.processEvents()\n",
    "\n",
    "tiPosX = np.array([])\n",
    "tiPosY = np.array([])\n",
    "\n",
    "Q = np.multiply(0.2,np.identity(4))\n",
    "R = np.multiply(5,np.array([[1],[1]]))\n",
    "\n",
    "for tlvStream in tlvData:\n",
    "    \n",
    "    #parsing\n",
    "    pointCloud, targetDict = tlvParsing(tlvStream, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes)\n",
    "    \n",
    "    if pointCloud.size > 0:\n",
    "        posX,posY,SNR = parsePointCloud(pointCloud) #dictionary that contains the point cloud data\n",
    "        s1.setData(posX, posY)\n",
    "        #initial noise reduction\n",
    "        clusters = TreeClusteringOnly(posX, posY, SNR,\n",
    "                                      weightThresholdIntial, minClusterSizeInitial)\n",
    "        \n",
    "        if clusters.size > 0:\n",
    "            \n",
    "            \n",
    "#             row 1 - x\n",
    "#             row 2 - y\n",
    "#             row 3 - SNR\n",
    "#             row 4 - cluster number\n",
    "            \n",
    "#             snr zone snr test\n",
    "#             4.5 to the end -> last zone\n",
    "#             3-4.5m -> middle zone\n",
    "#             1-3m -> first zone\n",
    "            snrMask_LastZone = np.logical_and(np.greater(clusters[1,:], 4.5), np.greater(clusters[2,:], snrLastZone)) #zone 4.5m and greater\n",
    "            snrMask_MiddleZone = np.logical_and(np.logical_and(np.greater(clusters[1,:], 3), np.less_equal(clusters[1,:], 4.5)), \n",
    "                                                np.greater(clusters[2,:], snrMiddleZone)) #zone 3-4.5m with SNR > 20\n",
    "            snrMask_FirstZone = np.logical_and(np.less_equal(clusters[1,:], 3), np.greater(clusters[2,:], snrFirstZone))\n",
    "            overallSnrMask = np.logical_or(np.logical_or(snrMask_FirstZone,snrMask_MiddleZone), snrMask_LastZone)\n",
    "\n",
    "            snrFilteredClusters = clusters[:,overallSnrMask]\n",
    "\n",
    "            if snrFilteredClusters.size > 0:\n",
    "                s2.setData(snrFilteredClusters[0,:], snrFilteredClusters[1,:])\n",
    "                \n",
    "                dbClusters = TreeClusteringOnly(snrFilteredClusters[0,:], snrFilteredClusters[1,:], \n",
    "                                                snrFilteredClusters[2,:], \n",
    "                                                weightThresholdFinal, minClusterSizeFinal)\n",
    "                if dbClusters.size > 0:\n",
    "                    #row 1 - x\n",
    "                    #row 2 - y\n",
    "                    #row 3 - cluster number\n",
    "                    k = int(max(dbClusters[3,:])) + 1 \n",
    "                    points = np.transpose(np.array([dbClusters[0,:], dbClusters[1,:]]))\n",
    "                  \n",
    "                    #kmeans \n",
    "                    centroidClusterer = KMeans(n_clusters= k).fit(points)\n",
    "                    centroidData = np.array([centroidClusterer.cluster_centers_[:,0], centroidClusterer.cluster_centers_[:,1]])\n",
    "\n",
    "                    #tracking\n",
    "                    centroidX, centroidP = LiveRKF(centroidData, centroidX, centroidP, Q, R)\n",
    "                    #plot\n",
    "                    #calculate x and y positions\n",
    "                    xPositions = np.multiply(centroidX[0,:], np.cos(centroidX[2,:]))\n",
    "                    yPositions = np.multiply(centroidX[0,:], np.sin(centroidX[2,:]))\n",
    "                   \n",
    "                    #calculate range and theta to remove from graphing\n",
    "#                     r = np.sqrt(np.add(np.square(xPositions),np.square(yPositions)))\n",
    "#                     theta = np.rad2deg(np.arctan(np.divide(yPositions,xPositions)))\n",
    "#                     positionMask = np.logical_and(np.less_equal(r,6), np.logical_and(np.less_equal(theta, 50), np.greater_equal(theta, -50)))\n",
    "                    \n",
    "#                     xPositions = xPositions[positionMask]\n",
    "#                     yPositions = yPositions[positionMask]\n",
    "                    \n",
    "                    s3.setData(xPositions, yPositions)\n",
    "\n",
    "    \n",
    "    if len(targetDict) != 0:\n",
    "        #kinematic data object structure\n",
    "        #row 0 - posX\n",
    "        #row 1 - posY \n",
    "        #row 2 - velX\n",
    "        #row 3 - velY\n",
    "        #row 4 - accX\n",
    "        #row 5 - accY\n",
    "        tiPosX = targetDict['kinematicData'][0,:]\n",
    "        tiPosY = targetDict['kinematicData'][1,:]\n",
    "        s4.setData(tiPosX,tiPosY)\n",
    "        \n",
    "    QtGui.QApplication.processEvents()\n",
    "    k = input('ENTER ')\n",
    "    \n",
    "#     time.sleep(0.05)\n",
    "    \n",
    "#     currentClustering = input('Expected Clustering: ')\n",
    "#     currentKalman = int(input('Expected Kalman: '))\n",
    "#     expectedClustering = np.append(expectedClustering,currentClustering)\n",
    "#     expectedKalman = np.append(expectedKalman,currentKalman)\n",
    "    \n",
    "\n",
    "#create pandas an output as csv\n",
    "groundTruthDf = pd.DataFrame({'expectedClustering':expectedClustering, 'expectedKalman':expectedKalman})\n",
    "groundTruthDf.to_csv('3PeopleWalkingGroundTruth.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
