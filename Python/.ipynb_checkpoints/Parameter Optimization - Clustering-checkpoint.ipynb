{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import zero_one_loss\n",
    "import pyqtgraph as pg\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "%gui qt5\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plottig GUI\n",
    "app = QtGui.QApplication([])\n",
    "pg.setConfigOption('background','w')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsWindow(title=\"Occupancy Detection GUI\")\n",
    "plot1 = win.addPlot()\n",
    "plot1.setXRange(-6,6)\n",
    "plot1.setYRange(0,6)\n",
    "plot1.setLabel('left',text = 'Y position (m)')\n",
    "plot1.setLabel('bottom', text= 'X position (m)')\n",
    "s1 = plot1.plot([],[],pen=None,symbol='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateChecksum(recieveHeader):\n",
    "    h = recieveHeader.view(dtype=np.uint16)\n",
    "    a = np.array([sum(h)], dtype=np.uint32)\n",
    "    b = np.array([sum(a.view(dtype=np.uint16))], dtype=np.uint16)\n",
    "    CS = np.uint16(~(b))\n",
    "    return CS\n",
    "\n",
    "def readHeader(recieveHeader):\n",
    "    headerContent = dict()\n",
    "    index = 0\n",
    "    \n",
    "    headerContent['magicBytes'] = recieveHeader[index:index+8]\n",
    "    index += 20\n",
    "    \n",
    "    headerContent['packetLength'] = recieveHeader[index:index+4].view(dtype=np.uint32)\n",
    "    index += 4\n",
    "        \n",
    "    headerContent['frameNumber'] = recieveHeader[index:index+4].view(dtype=np.uint32)\n",
    "    index += 24\n",
    "    \n",
    "    headerContent['numTLVs'] = recieveHeader[index:index+2].view(dtype=np.uint16)\n",
    "    \n",
    "    return headerContent\n",
    "\n",
    "def tlvParsing(data, dataLength, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes):\n",
    "    \n",
    "    targetDict = dict()\n",
    "    pointCloud = None\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index+4:index+8].view(dtype=np.uint32)\n",
    "    #TLV size check\n",
    "    if (tlvLength + index > dataLength):\n",
    "        print('TLV SIZE IS WRONG')\n",
    "        lostSync = True\n",
    "        return\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    pointCloudDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType == 6: #point cloud TLV\n",
    "        numberOfPoints = pointCloudDataLength/pointLengthInBytes\n",
    "        if numberOfPoints > 0:\n",
    "            p = data[index:index+pointCloudDataLength[0]].view(dtype=np.single)\n",
    "            #form the appropriate array \n",
    "            #each point is 16 bytes - 4 bytes for each property - range, azimuth, doppler, snr\n",
    "            pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "    \n",
    "    return pointCloud\n",
    "\n",
    "def liveParsing(tlvStream):\n",
    "    \n",
    "    tlvHeaderLengthInBytes = 8\n",
    "    pointLengthInBytes = 16\n",
    "    frameNumber = 0\n",
    "    \n",
    "    tlvStream = np.frombuffer(tlvStream, dtype = 'uint8')\n",
    "    #tlv header\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = tlvStream[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = tlvStream[index+4:index+8].view(dtype=np.uint32)\n",
    "\n",
    "    index += tlvHeaderLengthInBytes\n",
    "    tlvDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "\n",
    "    if tlvType == 6: \n",
    "        numberOfPoints = tlvDataLength/pointLengthInBytes\n",
    "        p = tlvStream[index:index+tlvDataLength[0]].view(np.single)\n",
    "        pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "\n",
    "        if not(pointCloud is None):\n",
    "            #constrain point cloud to within the effective sensor range\n",
    "            #range 1 < x < 6\n",
    "            #azimuth -50 deg to 50 deg\n",
    "            #check whether corresponding range and azimuth data are within the constraints\n",
    "\n",
    "            effectivePointCloud = np.array([])\n",
    "            for index in range(0, len(pointCloud[0,:])):\n",
    "                if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "                and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "                    and pointCloud[1, index] < 50*np.pi/180):\n",
    "\n",
    "                    #concatenate columns to the new point cloud\n",
    "                    if len(effectivePointCloud) == 0:\n",
    "                        effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "                    else:\n",
    "                        point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                        effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "\n",
    "            if len(effectivePointCloud) != 0:\n",
    "                posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "                posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "                return posX, posY\n",
    "            \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterativeDfs(vertexID, edgeMatrix, startNode):\n",
    "    \n",
    "    visited = np.array([], dtype=np.int)\n",
    "    dfsStack = np.array([startNode])\n",
    "\n",
    "    while np.logical_not(np.equal(dfsStack.size,0)):\n",
    "        vertex, dfsStack = dfsStack[-1], dfsStack[:-1] #equivalent to stack pop function\n",
    "        if vertex not in visited:\n",
    "            #find unvisited nodes\n",
    "            unvisitedNodes = vertexID[np.logical_not(np.isnan(edgeMatrix[int(vertex), :]))]\n",
    "            visited = np.append(visited, vertex)\n",
    "            #add unvisited nodes to the stack\n",
    "            dfsStack = np.append(dfsStack, unvisitedNodes[np.logical_not(np.isin(unvisitedNodes,visited))])\n",
    "    \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombinationClustering(posX, posY):\n",
    "    weightThreshold = 0.5 #minimum distance between points\n",
    "    minClusterSize = 20\n",
    "    \n",
    "    vertexID = np.arange(len(posX))\n",
    "    vertexList = np.arange(len(posX))\n",
    "    \n",
    "    xMean = np.array([])\n",
    "    yMean = np.array([])\n",
    "\n",
    "    if len(posX) >= minClusterSize:\n",
    "        edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "\n",
    "        #create distance matrix\n",
    "        #x1 - x0\n",
    "        xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                                  np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "        #y1 - y0\n",
    "        yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                                  np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "        #euclidean distance calculation\n",
    "        edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "        #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "        weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "        edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "        #perform iterative dfs\n",
    "        pointsX = np.array([])\n",
    "        pointsY = np.array([])\n",
    "\n",
    "        centroidNumber = 0\n",
    "        while vertexID.size > 0:\n",
    "            startNode = vertexID[0]\n",
    "            visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "            #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "            vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "            #visited is a component, extract cluster from it if possible\n",
    "            if visited.size >= minClusterSize:\n",
    "                pointsX = np.append(pointsX, posX[visited])\n",
    "                pointsY = np.append(pointsY, posY[visited]) \n",
    "\n",
    "        if pointsX.size > 0:\n",
    "            clusterer = DBSCAN(eps=0.7, min_samples=25)\n",
    "            clusterer.fit(pd.DataFrame(np.transpose(np.array([pointsX,pointsY]))).values)\n",
    "\n",
    "            if clusterer.core_sample_indices_.size > 0:\n",
    "                #array that contains the x,y positions and the cluster association number\n",
    "                clusters = np.array([pointsX[clusterer.core_sample_indices_],\n",
    "                          pointsY[clusterer.core_sample_indices_], \n",
    "                         clusterer.labels_[clusterer.core_sample_indices_]])\n",
    "                for centroidNumber in np.unique(clusters[2,:]):\n",
    "                    xMean = np.append(xMean, np.mean(clusters[0,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "                    yMean = np.append(yMean, np.mean(clusters[1,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "    return yMean, xMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCANOnlyClustering(pointsX, pointsY, epsValue, minSamplesValue):\n",
    "    \n",
    "    #initialize constraints/variables\n",
    "    minClusterSize = 1\n",
    "    xMean = np.array([])\n",
    "    yMean = np.array([])\n",
    "    \n",
    "    if len(pointsX) >= minClusterSize:\n",
    "\n",
    "        clusterer = DBSCAN(eps=epsValue, min_samples=minSamplesValue)\n",
    "        \n",
    "        clusterer.fit(pd.DataFrame(np.transpose(np.array([pointsX,pointsY]))).values)\n",
    "\n",
    "        if clusterer.core_sample_indices_.size > 0:\n",
    "            #array that contains the x,y positions and the cluster association number\n",
    "            clusters = np.array([pointsX[clusterer.core_sample_indices_],\n",
    "                      pointsY[clusterer.core_sample_indices_], \n",
    "                     clusterer.labels_[clusterer.core_sample_indices_]])\n",
    "            for centroidNumber in np.unique(clusters[2,:]):\n",
    "                xMean = np.append(xMean, np.mean(clusters[0,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "                yMean = np.append(yMean, np.mean(clusters[1,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "                \n",
    "\n",
    "\n",
    "    return yMean, xMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data (mat file)\n",
    "parsingMatFile = 'C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Experiment Data 2\\\\3PeopleWalking.mat'\n",
    "tlvData = (loadmat(parsingMatFile))['tlvStream'][0]\n",
    "\n",
    "#for one person walking location based\n",
    "trueLabels = np.repeat(3, repeats=len(tlvData))\n",
    "\n",
    "#parameter sweep\n",
    "eps = np.arange(start=0.1, stop=0.8, step=0.05)\n",
    "minSamples = np.arange(start=2, stop=30, step=1)\n",
    "\n",
    "parameterInformation = np.array([])\n",
    "\n",
    "for epsValue in eps:\n",
    "    for minSampleValue in minSamples:\n",
    "        predictedLabels = np.array([])\n",
    "        for tlvStream in tlvData:\n",
    "            #parse\n",
    "            posX, posY = liveParsing(tlvStream)\n",
    "            try:\n",
    "                if (posX == None).all():\n",
    "                    predictedLabels = np.append(predictedLabels, np.array([0]))\n",
    "            except AttributeError:\n",
    "                predictedLabels = np.append(predictedLabels, np.array([0]))\n",
    "                continue\n",
    "            #cluster\n",
    "            yMean, xMean = DBSCANOnlyClustering(posX, posY, epsValue, minSampleValue)\n",
    "            predictedInstance = np.array([len(yMean)])\n",
    "            predictedLabels = np.append(predictedLabels, predictedInstance)\n",
    "        #calculate rmse\n",
    "        misclassifications = zero_one_loss(trueLabels, predictedLabels, normalize=False)\n",
    "        if len(parameterInformation) == 0:\n",
    "            parameterInformation = np.array([epsValue, minSampleValue, misclassifications])\n",
    "        else:\n",
    "            parameterInformation = np.vstack((parameterInformation,np.array([epsValue, minSampleValue, misclassifications])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save parameter information as csv\n",
    "DBSCANDf = pd.DataFrame(parameterInformation, columns=['Eps','MinSamples', 'Misclassifications'])\n",
    "DBSCANDf.to_csv('ClusteringParameterInfo_3PeopleWalking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eps</th>\n",
       "      <th>MinSamples</th>\n",
       "      <th>Misclassifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Eps  MinSamples  Misclassifications\n",
       "336  0.70         2.0               104.0\n",
       "364  0.75         2.0               104.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find best parameter set - Dhari\n",
    "DBSCANDf[DBSCANDf['Misclassifications']==np.min(DBSCANDf['Misclassifications'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eps</th>\n",
       "      <th>MinSamples</th>\n",
       "      <th>Misclassifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Eps  MinSamples  Misclassifications\n",
       "364  0.75         2.0                68.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find best parameter set - Alan\n",
    "DBSCANDf[DBSCANDf['Misclassifications']==np.min(DBSCANDf['Misclassifications'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Eps  MinSamples  Misclassifications\n",
      "168  0.4         2.0               247.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eps</th>\n",
       "      <th>MinSamples</th>\n",
       "      <th>Misclassifications</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2.0</td>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Eps  MinSamples  Misclassifications\n",
       "364  0.75         2.0               257.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find best parameter set - 2 People Walking\n",
    "print(DBSCANDf[DBSCANDf['Misclassifications']==np.min(DBSCANDf['Misclassifications'])])\n",
    "DBSCANDf.loc[[364]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Eps  MinSamples  Misclassifications\n",
      "224  0.5         2.0                93.0\n",
      "\n",
      "      Eps  MinSamples  Misclassifications\n",
      "364  0.75         2.0               132.0\n"
     ]
    }
   ],
   "source": [
    "#find best parameter set - 3PeopleWalking\n",
    "print(DBSCANDf[DBSCANDf['Misclassifications']==np.min(DBSCANDf['Misclassifications'])])\n",
    "print()\n",
    "print(DBSCANDf.loc[[364]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
