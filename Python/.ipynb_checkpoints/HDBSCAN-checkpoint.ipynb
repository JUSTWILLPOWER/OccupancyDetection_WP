{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "#pyqtgraph -> fast plotting\n",
    "import pyqtgraph as pg\n",
    "from pyqtgraph.Qt import QtGui\n",
    "%gui qt5\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "<ul>\n",
    "    <li> \n",
    "        min_cluster_size - smallest size grouping that you wish to consider a cluster\n",
    "    </li>\n",
    "    <li> \n",
    "        min_samples - a measure of how conservative you want you clustering to be. The larger \n",
    "        the value of min_samples you provide, the more conservative the clustering â€“ more points will be \n",
    "        declared as noise, and clusters will be restricted to progressively more dense areas.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise variables\n",
    "lostSync = False\n",
    "\n",
    "#valid header variables and constant\n",
    "magicBytes = np.array([2,1,4,3,6,5,8,7], dtype= 'uint8')\n",
    "\n",
    "isMagicOk = False\n",
    "isDataOk = False\n",
    "gotHeader = False\n",
    "\n",
    "frameHeaderLength = 52 #52 bytes long\n",
    "tlvHeaderLengthInBytes = 8\n",
    "pointLengthInBytes = 16\n",
    "frameNumber = 1\n",
    "targetFrameNumber = 0\n",
    "targetLengthInBytes = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one person slow walk\n",
    "#TLV Data Load\n",
    "tlvData = (loadmat('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\slowWalk.mat'))['tlvStream'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract people walking \n",
    "walking = tlvData[200:500]\n",
    "tlvData = walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two people slow walk\n",
    "#TLV Data Load\n",
    "tlvData = (loadmat('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\2PeopleMoving.mat'))['tlvStream'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three people walk\n",
    "#TLV Data Load\n",
    "tlvData = (loadmat('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\3PeopleWalking.mat'))['tlvStream'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = QtGui.QApplication([])\n",
    "pg.setConfigOption('background','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsWindow(title=\"\")\n",
    "plot1 = win.addPlot()\n",
    "plot1.setXRange(-6,6)\n",
    "plot1.setYRange(0,6)\n",
    "plot1.setLabel('left',text = 'Y position (m)')\n",
    "plot1.setLabel('bottom', text= 'X position (m)')\n",
    "s1 = plot1.plot([],[],pen=None,symbol='o')\n",
    "\n",
    "plot2 = win.addPlot()\n",
    "plot2.setXRange(-6,6)\n",
    "plot2.setYRange(0,6)\n",
    "plot2.setLabel('left',text = 'Y position (m)')\n",
    "plot2.setLabel('bottom', text= 'X position (m)')\n",
    "s2 = plot2.plot([],[],pen=None,symbol='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroidData = list();\n",
    "for tlvStream in tlvData:\n",
    "    tlvStream = np.frombuffer(tlvStream, dtype = 'uint8')\n",
    "    \n",
    "    #tlv header\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = tlvStream[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = tlvStream[index+4:index+8].view(dtype=np.uint32)\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    tlvDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "\n",
    "    if tlvType == 6: \n",
    "        numberOfPoints = tlvDataLength/pointLengthInBytes\n",
    "        p = tlvStream[index:index+tlvDataLength[0]].view(np.single)\n",
    "        pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "\n",
    "        if not(pointCloud is None):\n",
    "            #constrain point cloud to within the effective sensor range\n",
    "            #range 1 < x < 6\n",
    "            #azimuth -50 deg to 50 deg\n",
    "            #check whether corresponding range and azimuth data are within the constraints\n",
    "\n",
    "            effectivePointCloud = np.array([])\n",
    "            for index in range(0, len(pointCloud[0,:])):\n",
    "                if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "                and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "                     and pointCloud[1, index] < 50*np.pi/180):\n",
    "        \n",
    "                    #concatenate columns to the new point cloud\n",
    "                    if len(effectivePointCloud) == 0:\n",
    "                        effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "                    else:\n",
    "                        point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                        effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "                        \n",
    "\n",
    "            if len(effectivePointCloud) != 0:\n",
    "                posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "                posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "                positions = pd.DataFrame({'X':posX, 'Y':posY})\n",
    "                if len(positions.values) > 5:\n",
    "                    clusterer = DBSCAN(eps=0.2, min_samples=18)\n",
    "                    clusterer.fit(positions.values)\n",
    "\n",
    "                    #extract clusters\n",
    "                    s1.setData(positions.values[clusterer.core_sample_indices_])\n",
    "                    QtGui.QApplication.processEvents() \n",
    "                    time.sleep(0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stores the cluster information\n",
    "#cluster number for each data sample\n",
    "#note that -1 is noise\n",
    "clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of clusters\n",
    "clusterer.labels_.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63928435, 0.54577043, 0.68495415, ..., 0.75363376, 0.84837895,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hdbscan implements soft clustering - where each data point is assigned a cluster membership score ranging \n",
    "# from 0.0 to 1.0. A score of 0.0 represents a sample that is not in the cluster at all \n",
    "# (all noise points will get this score) while a score of 1.0 represents a sample that is \n",
    "# at the heart of the cluster (note that this is not the spatial centroid notion of core).\n",
    "\n",
    "clusterer.probabilities_\n",
    "\n",
    "#probabiity for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hbscan allows us to data associate using predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
