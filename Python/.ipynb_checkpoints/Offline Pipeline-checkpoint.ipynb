{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import pyqtgraph as pg\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "%gui qt5\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plottig GUI\n",
    "app = QtGui.QApplication([])\n",
    "pg.setConfigOption('background','w')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsWindow(title=\"Occupancy Detection GUI\")\n",
    "plot1 = win.addPlot()\n",
    "plot1.setXRange(-6,6)\n",
    "plot1.setYRange(0,6)\n",
    "plot1.setLabel('left',text = 'Y position (m)')\n",
    "plot1.setLabel('bottom', text= 'X position (m)')\n",
    "s1 = plot1.plot([],[],pen=None,symbol='o')\n",
    "plot2 = win.addPlot()\n",
    "plot2.setXRange(-6,6)\n",
    "plot2.setYRange(0,6)\n",
    "plot2.setLabel('left',text = 'Y position (m)')\n",
    "plot2.setLabel('bottom', text= 'X position (m)')\n",
    "s2 = plot2.plot([],[],pen=None,symbol='o')\n",
    "plot3 = win.addPlot()\n",
    "plot3.setXRange(-6,6)\n",
    "plot3.setYRange(0,6)\n",
    "plot3.setLabel('left',text = 'Y position (m)')\n",
    "plot3.setLabel('bottom', text= 'X position (m)')\n",
    "s3 = plot3.plot([],[],pen=None,symbol='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateChecksum(recieveHeader):\n",
    "    h = recieveHeader.view(dtype=np.uint16)\n",
    "    a = np.array([sum(h)], dtype=np.uint32)\n",
    "    b = np.array([sum(a.view(dtype=np.uint16))], dtype=np.uint16)\n",
    "    CS = np.uint16(~(b))\n",
    "    return CS\n",
    "\n",
    "def readHeader(recieveHeader):\n",
    "    headerContent = dict()\n",
    "    index = 0\n",
    "    \n",
    "    headerContent['magicBytes'] = recieveHeader[index:index+8]\n",
    "    index += 20\n",
    "    \n",
    "    headerContent['packetLength'] = recieveHeader[index:index+4].view(dtype=np.uint32)\n",
    "    index += 4\n",
    "        \n",
    "    headerContent['frameNumber'] = recieveHeader[index:index+4].view(dtype=np.uint32)\n",
    "    index += 24\n",
    "    \n",
    "    headerContent['numTLVs'] = recieveHeader[index:index+2].view(dtype=np.uint16)\n",
    "    \n",
    "    return headerContent\n",
    "\n",
    "def tlvParsing(data, dataLength, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes):\n",
    "    \n",
    "    targetDict = dict()\n",
    "    pointCloud = None\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index+4:index+8].view(dtype=np.uint32)\n",
    "    #TLV size check\n",
    "    if (tlvLength + index > dataLength):\n",
    "        print('TLV SIZE IS WRONG')\n",
    "        lostSync = True\n",
    "        return\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    pointCloudDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType == 6: #point cloud TLV\n",
    "        numberOfPoints = pointCloudDataLength/pointLengthInBytes\n",
    "        if numberOfPoints > 0:\n",
    "            p = data[index:index+pointCloudDataLength[0]].view(dtype=np.single)\n",
    "            #form the appropriate array \n",
    "            #each point is 16 bytes - 4 bytes for each property - range, azimuth, doppler, snr\n",
    "            pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "    \n",
    "    return pointCloud\n",
    "\n",
    "def liveParsing(tlvStream):\n",
    "    \n",
    "    tlvHeaderLengthInBytes = 8\n",
    "    pointLengthInBytes = 16\n",
    "    frameNumber = 0\n",
    "    \n",
    "    tlvStream = np.frombuffer(tlvStream, dtype = 'uint8')\n",
    "    #tlv header\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = tlvStream[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = tlvStream[index+4:index+8].view(dtype=np.uint32)\n",
    "\n",
    "    index += tlvHeaderLengthInBytes\n",
    "    tlvDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "\n",
    "    if tlvType == 6: \n",
    "        numberOfPoints = tlvDataLength/pointLengthInBytes\n",
    "        p = tlvStream[index:index+tlvDataLength[0]].view(np.single)\n",
    "        pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "\n",
    "        if not(pointCloud is None):\n",
    "            #constrain point cloud to within the effective sensor range\n",
    "            #range 1 < x < 6\n",
    "            #azimuth -50 deg to 50 deg\n",
    "            #check whether corresponding range and azimuth data are within the constraints\n",
    "\n",
    "            effectivePointCloud = np.array([])\n",
    "            for index in range(0, len(pointCloud[0,:])):\n",
    "                if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "                and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "                    and pointCloud[1, index] < 50*np.pi/180):\n",
    "\n",
    "                    #concatenate columns to the new point cloud\n",
    "                    if len(effectivePointCloud) == 0:\n",
    "                        effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "                    else:\n",
    "                        point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                        effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "\n",
    "            if len(effectivePointCloud) != 0:\n",
    "                posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "                posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "                SNR  = effectivePointCloud[3,:]\n",
    "                return posX, posY, SNR\n",
    "            \n",
    "    return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterativeDfs(vertexID, edgeMatrix, startNode):\n",
    "    \n",
    "    visited = np.array([], dtype=np.int)\n",
    "    dfsStack = np.array([startNode])\n",
    "\n",
    "    while np.logical_not(np.equal(dfsStack.size,0)):\n",
    "        vertex, dfsStack = dfsStack[-1], dfsStack[:-1] #equivalent to stack pop function\n",
    "        if vertex not in visited:\n",
    "            #find unvisited nodes\n",
    "            unvisitedNodes = vertexID[np.logical_not(np.isnan(edgeMatrix[int(vertex), :]))]\n",
    "            visited = np.append(visited, vertex)\n",
    "            #add unvisited nodes to the stack\n",
    "            dfsStack = np.append(dfsStack, unvisitedNodes[np.logical_not(np.isin(unvisitedNodes,visited))])\n",
    "    \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombinationClustering(posX, posY, SNR):\n",
    "    weightThreshold = 0.8 #minimum distance between points\n",
    "    minClusterSize = 3\n",
    "    \n",
    "    vertexID = np.arange(len(posX))\n",
    "    vertexList = np.arange(len(posX))\n",
    "    \n",
    "    xMean = np.array([])\n",
    "    yMean = np.array([])\n",
    "\n",
    "    if len(posX) >= minClusterSize:\n",
    "        edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "\n",
    "        #create distance matrix\n",
    "        #x1 - x0\n",
    "        xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                                  np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "        #y1 - y0\n",
    "        yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                                  np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "        #euclidean distance calculation\n",
    "        edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "        #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "        weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "        edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "        #perform iterative dfs\n",
    "        pointsX = np.array([])\n",
    "        pointsY = np.array([])\n",
    "\n",
    "        centroidNumber = 0\n",
    "        while vertexID.size > 0:\n",
    "            startNode = vertexID[0]\n",
    "            visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "            #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "            vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "            #visited is a component, extract cluster from it if possible\n",
    "            if visited.size >= minClusterSize:\n",
    "                pointsX = np.append(pointsX, posX[visited])\n",
    "                pointsY = np.append(pointsY, posY[visited]) \n",
    "\n",
    "        if pointsX.size > 0:\n",
    "            clusterer = DBSCAN(eps=1, min_samples=5)\n",
    "            clusterer.fit(pd.DataFrame(np.transpose(np.array([pointsX,pointsY]))).values)\n",
    "\n",
    "            if clusterer.core_sample_indices_.size > 0:\n",
    "                #array that contains the x,y positions and the cluster association number\n",
    "                clusters = np.array([pointsX[clusterer.core_sample_indices_],\n",
    "                          pointsY[clusterer.core_sample_indices_], \n",
    "                         clusterer.labels_[clusterer.core_sample_indices_]])\n",
    "                for centroidNumber in np.unique(clusters[2,:]):\n",
    "                    xMean = np.append(xMean, np.mean(clusters[0,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "                    yMean = np.append(yMean, np.mean(clusters[1,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "    return yMean, xMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCANOnlyClusteringWeightedSum(pointsX, pointsY):\n",
    "    \n",
    "    #initialize constraints/variables\n",
    "    minClusterSize = 1\n",
    "    xMean = np.array([])\n",
    "    yMean = np.array([])\n",
    "    \n",
    "    if len(pointsX) >= minClusterSize:\n",
    "\n",
    "        clusterer = DBSCAN(eps=1, min_samples=5)\n",
    "        \n",
    "        clusterer.fit(pd.DataFrame(np.transpose(np.array([pointsX,pointsY]))).values)\n",
    "\n",
    "        if clusterer.core_sample_indices_.size > 0:\n",
    "            #array that contains the x,y positions and the cluster association number\n",
    "            clusters = np.array([pointsX[clusterer.core_sample_indices_],\n",
    "                      pointsY[clusterer.core_sample_indices_], \n",
    "                     clusterer.labels_[clusterer.core_sample_indices_]])\n",
    "            for centroidNumber in np.unique(clusters[2,:]):\n",
    "                xMean = np.append(xMean, np.mean(clusters[0,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "                yMean = np.append(yMean, np.mean(clusters[1,:][np.isin(clusters[2,:], centroidNumber)]))\n",
    "                \n",
    "\n",
    "\n",
    "    return yMean, xMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCANOnly(pointsX, pointsY):\n",
    "    \n",
    "    #initialize constraints/variables\n",
    "    minClusterSize = 1\n",
    "    xMean = np.array([])\n",
    "    yMean = np.array([])\n",
    "    \n",
    "    if len(pointsX) >= minClusterSize:\n",
    "\n",
    "        clusterer = DBSCAN(eps=1, min_samples=5)\n",
    "        \n",
    "        clusterer.fit(pd.DataFrame(np.transpose(np.array([pointsX,pointsY]))).values)\n",
    "\n",
    "        if clusterer.core_sample_indices_.size > 0:\n",
    "            #array that contains the x,y positions and the cluster association number\n",
    "            clusters = np.array([pointsX[clusterer.core_sample_indices_],\n",
    "                      pointsY[clusterer.core_sample_indices_], \n",
    "                     clusterer.labels_[clusterer.core_sample_indices_]])\n",
    "                \n",
    "            \n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreeClusteringOnly(posX, posY, SNR):\n",
    "    weightThreshold = 1 #minimum distance between points\n",
    "    minClusterSize = 15\n",
    "    \n",
    "    vertexID = np.arange(len(posX))\n",
    "    vertexList = np.arange(len(posX))\n",
    "    \n",
    "    xMean = np.array([])\n",
    "    yMean = np.array([])\n",
    "\n",
    "    if len(posX) >= minClusterSize:\n",
    "        edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "\n",
    "        #create distance matrix\n",
    "        #x1 - x0\n",
    "        xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                                  np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "        #y1 - y0\n",
    "        yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                                  np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "        #euclidean distance calculation\n",
    "        edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "        #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "        weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "        edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "        #perform iterative dfs\n",
    "        associatedPoints = np.array([])\n",
    "        \n",
    "        \n",
    "        centroidNumber = 0\n",
    "        asscociatedPoints = np.array([])\n",
    "        while vertexID.size > 0:\n",
    "            startNode = vertexID[0]\n",
    "            visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "            #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "            vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "#             #visited is a component, extract cluster from it if possible\n",
    "            if visited.size >= minClusterSize:\n",
    "                cluster =  np.array([posX[visited], posY[visited],SNR[visited],\n",
    "                                     np.repeat(centroidNumber, repeats=len(visited))])\n",
    "                if asscociatedPoints.size == 0:\n",
    "                    asscociatedPoints = cluster\n",
    "                else:\n",
    "                    asscociatedPoints = np.hstack((asscociatedPoints, cluster))\n",
    "                centroidNumber += 1\n",
    "                                    \n",
    "                \n",
    "\n",
    "\n",
    "        return asscociatedPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, P, A, Q): #predict function\n",
    "    xpred = np.matmul(A,x)\n",
    "    Ppred = np.matmul(A,P)\n",
    "    Ppred = np.matmul(Ppred,np.transpose(A)) + Q\n",
    "    return(xpred, Ppred)\n",
    "\n",
    "def innovation(xpred, Ppred, z, H, R): #innovation function\n",
    "    nu = z - np.matmul(H,xpred)\n",
    "    S = np.matmul(H,Ppred)\n",
    "    S = R + np.matmul(S, np.transpose(H))\n",
    "    return(nu, S)\n",
    "\n",
    "def innovation_update(xpred, Ppred, nu, S, H):\n",
    "    K = np.matmul(Ppred, np.transpose(H))\n",
    "    K = np.matmul(K,np.linalg.inv(S)) #check inverse function\n",
    "    xnew = xpred + np.matmul(K,nu)\n",
    "    Pnew = np.matmul(K,S)\n",
    "    Pnew = Ppred - np.matmul(Pnew,np.transpose(K)) \n",
    "    return(xnew, Pnew)\n",
    "\n",
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return(rho, phi)\n",
    "\n",
    "def data_associate(centroidPred, rthetacentroid):\n",
    "    rthetacentroidCurrent = rthetacentroid\n",
    "    centpredCol = np.size(centroidPred,1)\n",
    "    rthetaCol = np.size(rthetacentroid,1)\n",
    "\n",
    "    for i in list(range(0,centpredCol)):\n",
    "        r1 = centroidPred[0][i]\n",
    "        r2 = rthetacentroid[0]\n",
    "        theta1 = centroidPred[2][i]\n",
    "        theta2 = rthetacentroid[1]\n",
    "        temp = np.sqrt(np.multiply(r1,r1) + np.multiply(r2,r2) - np.multiply(np.multiply(np.multiply(2,r1),r2),np.cos(theta2-theta1)))\n",
    "        if(i==0):\n",
    "            minDist = temp\n",
    "        else:\n",
    "            minDist = np.vstack((minDist,temp))\n",
    "\n",
    "    currentFrame = np.empty((2,max(centpredCol,rthetaCol)))\n",
    "    currentFrame[:] = np.nan\n",
    "\n",
    "    minDist = np.reshape(minDist, (centpredCol,rthetaCol))\n",
    "    minDistOrg = minDist\n",
    "\n",
    "    for i in list(range(0,min(centpredCol,rthetaCol))):\n",
    "        if((np.ndim(minDist)) == 1):\n",
    "            minDist = np.reshape(minDist,(rthetaCol,1))\n",
    "            minDistOrg = np.reshape(minDistOrg,(rthetaCol,1))\n",
    "        val = np.min(minDist)\n",
    "        resultOrg = np.argwhere(minDistOrg == val)\n",
    "        result = np.argwhere(minDist == val)\n",
    "        minRowOrg = resultOrg[0][0]\n",
    "        minColOrg = resultOrg[0][1]\n",
    "        minRow = result[0][0]\n",
    "        minCol = result[0][1]\n",
    "        currentFrame[:,minRowOrg] = rthetacentroid[:,minColOrg]\n",
    "        minDist = np.delete(minDist,minRow,0)\n",
    "        minDist = np.delete(minDist,minCol,1)\n",
    "        rthetacentroidCurrent = np.delete(rthetacentroidCurrent,minCol,1)\n",
    "\n",
    "    index = 0\n",
    "    if (rthetacentroidCurrent.size != 0): #check indexing\n",
    "        for i in list(range(centpredCol,rthetaCol)):\n",
    "            currentFrame[:,i] = rthetacentroidCurrent[:,index]\n",
    "            index += 1 \n",
    "\n",
    "    return(currentFrame)\n",
    "\n",
    "def LiveRKF(currentrawxycentroidData, centroidX, centroidP):\n",
    "    \n",
    "    #initialise matrices \n",
    "    delT = 0.0500\n",
    "    A = np.array([[1,delT,0,0], \n",
    "                  [0,1,0,0], \n",
    "                  [0,0,1,delT], \n",
    "                  [0,0,0,1]])\n",
    "    H = np.array([[1,0,0,0],\n",
    "                  [0,0,1,0]])\n",
    "    P = np.identity(4)\n",
    "    Q = np.multiply(0.9,np.identity(4))\n",
    "    R = np.array([[1],[1]])\n",
    "\n",
    "    xytransposecentroidData = currentrawxycentroidData\n",
    "    rthetacentroidData=xytransposecentroidData\n",
    "    if (xytransposecentroidData.size != 0): \n",
    "        [rthetacentroidData[0,:],rthetacentroidData[1,:]] = cart2pol(xytransposecentroidData[0,:],xytransposecentroidData[1,:])\n",
    "    if((rthetacentroidData.size != 0)):\n",
    "        currentFrame = data_associate(centroidX, rthetacentroidData)\n",
    "        addittionalCentroids = (np.size(rthetacentroidData,1)-np.size(centroidX,1))\n",
    "        if(addittionalCentroids>0):\n",
    "            centroidX = np.pad(centroidX, ((0,0),(0,addittionalCentroids)), 'constant') #initialises previous iteration to zer\n",
    "            for newFrameIndex in list((range(0, addittionalCentroids))):\n",
    "                centroidP.extend([P])\n",
    "        for currentFrameIndex in list((range(0,np.size(currentFrame,1)))):\n",
    "            if(not(np.isnan(currentFrame[0,currentFrameIndex]))):\n",
    "                [xpred, Ppred] = predict(centroidX[:,currentFrameIndex], centroidP[currentFrameIndex], A, Q)\n",
    "                [nu, S] = innovation(xpred, Ppred, currentFrame[:, currentFrameIndex], H, R)\n",
    "                [centroidX[:,currentFrameIndex],  centroidP[currentFrameIndex]] = innovation_update(xpred, Ppred, nu, S, H)\n",
    "            else:\n",
    "                [centroidX[:,currentFrameIndex], centroidP[currentFrameIndex]] = predict(centroidX[:,currentFrameIndex], centroidP[currentFrameIndex], A, Q)                   \n",
    "    else:\n",
    "        for noFrameIndex in list((range(0,np.size(centroidX,1)))):\n",
    "            [centroidX[:,noFrameIndex], centroidP[noFrameIndex]] = predict(centroidX[:,noFrameIndex], centroidP[noFrameIndex], A, Q)\n",
    "            \n",
    "    #centroidX is 4xN array that contains that centroid information for that frame\n",
    "    return centroidX, centroidP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Offline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# parsingMatFile = 'C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Experiment Data 2\\\\1PersonWalkingLocationDhari.mat'\n",
    "parsingMatFile = 'C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\fastWalk.mat'\n",
    "\n",
    "tlvData = (loadmat(parsingMatFile))['tlvStream'][0]\n",
    "\n",
    "centroidX =np.zeros((4,1))\n",
    "centroidP = []\n",
    "P = np.identity(4);\n",
    "centroidP.extend([P])\n",
    "\n",
    "snrThreshold = 20\n",
    "numberOfSucessfulClusters = 0\n",
    "\n",
    "for tlvStream in tlvData:\n",
    "    #parse\n",
    "    posX, posY, SNR = liveParsing(tlvStream)\n",
    "\n",
    "    try:\n",
    "        if (posX == None).all():\n",
    "            continue\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    \n",
    "    #tree based clustering\n",
    "    clusters = TreeClusteringOnly(posX, posY, SNR)\n",
    "    if clusters is not None and clusters.size > 0:\n",
    "        #where Row 0 - X\n",
    "        #Row 1 - Y\n",
    "        #Row 2 - SNR \n",
    "        #Row 3 - Cluster Identifier (just an integer to distinguish points associated to different clusters)\n",
    "        s1.setData(posX, posY)\n",
    "        s2.setData(clusters[0,:], clusters[1,:])\n",
    "\n",
    "        #SNR testing - remove weak points\n",
    "        snrMask = np.greater(clusters[2,:], snrThreshold)\n",
    "        filteredClusters = clusters[:,snrMask]\n",
    "        if filteredClusters[3,:].size > 0:\n",
    "            s3.setData(filteredClusters[0,:], filteredClusters[1,:])\n",
    "            print(max(filteredClusters[3,:]))\n",
    "        else:\n",
    "            s3.setData([], [])\n",
    "        QtGui.QApplication.processEvents() \n",
    "        time.sleep(0.05)\n",
    "\n",
    "#     #SNR testing - remove weak points\n",
    "#     snrMask = np.greater(SNR, snrThreshold)\n",
    "#     snrX = posX[snrMask]\n",
    "#     snrY = posY[snrMask]\n",
    "    \n",
    "#     s1.setData(posX,posY)\n",
    "#     s2.setData(snrX,snrY)\n",
    "#     yMean, xMean  = CombinationClustering(snrX, snrY)\n",
    "#     if yMean.size > 1:\n",
    "#         print('HALT')\n",
    "#         break\n",
    "# #     print(yMean, xMean)\n",
    "#     s3.setData(xMean, yMean)\n",
    "#     QtGui.QApplication.processEvents() \n",
    "#     time.sleep(0.05)\n",
    "    \n",
    "        \n",
    "#     #cluster\n",
    "#     yMean, xMean = CombinationClustering(posX, posY)\n",
    "    \n",
    "#     centroidData = np.array([xMean, yMean])\n",
    "#     #track\n",
    "#     centroidX, centroidP = LiveRKF(centroidData, centroidX, centroidP)\n",
    "#     #plot\n",
    "#     #calculate x and y positions\n",
    "#     xPositions = np.multiply(centroidX[0,:], np.cos(centroidX[2,:]))\n",
    "#     yPositions = np.multiply(centroidX[0,:], np.sin(centroidX[2,:]))\n",
    "#     numberOfTargets = len(xPositions)\n",
    "#     s1.setData(xPositions, yPositions)\n",
    "#     message = \"Occupancy Estimate: \" + str(numberOfTargets)\n",
    "#     win.removeItem(occupancyEstimate)\n",
    "#     occupancyEstimate = win.addLabel(text=message)\n",
    "#     QtGui.QApplication.processEvents() \n",
    "#     time.sleep(0.05)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 6)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(posX, posY)\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-6,6])\n",
    "axes.set_ylim([0,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to get predicted data to compare against labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsingMatFile = 'C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Experiment Data 2\\\\OnePersonWalkingLocation.mat'\n",
    "tlvData = (loadmat(parsingMatFile))['tlvStream'][0]\n",
    "\n",
    "predictedLabels = np.array([])\n",
    "\n",
    "for tlvStream in tlvData:\n",
    "    #parse\n",
    "    posX, posY = liveParsing(tlvStream)\n",
    "    try:\n",
    "        if (posX == None).all():\n",
    "            continue\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    #cluster\n",
    "    yMean, xMean = CombinationClustering(posX, posY)\n",
    "    predictedLabels = np.append(predictedLabels, len(yMean))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5150152274540494"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
