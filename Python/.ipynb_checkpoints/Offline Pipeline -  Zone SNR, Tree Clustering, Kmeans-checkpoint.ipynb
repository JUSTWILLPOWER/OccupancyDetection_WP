{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pyqtgraph as pg\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "%gui qt5\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import numba as nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plottig GUI\n",
    "app = QtGui.QApplication([])\n",
    "pg.setConfigOption('background','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsWindow(title=\"Occupancy Detection GUI\")\n",
    "plot1 = win.addPlot()\n",
    "plot1.setXRange(-6,6)\n",
    "plot1.setYRange(0,6)\n",
    "plot1.setLabel('left',text = 'Y position (m)')\n",
    "plot1.setLabel('bottom', text= 'X position (m)')\n",
    "s1 = plot1.plot([],[],pen=None,symbol='o')\n",
    "plot2 = win.addPlot()\n",
    "plot2.setXRange(-6,6)\n",
    "plot2.setYRange(0,6)\n",
    "plot2.setLabel('left',text = 'Y position (m)')\n",
    "plot2.setLabel('bottom', text= 'X position (m)')\n",
    "s2 = plot2.plot([],[],pen=None,symbol='o')\n",
    "plot3 = win.addPlot()\n",
    "plot3.setXRange(-6,6)\n",
    "plot3.setYRange(0,6)\n",
    "plot3.setLabel('left',text = 'Y position (m)')\n",
    "plot3.setLabel('bottom', text= 'X position (m)')\n",
    "s3 = plot3.plot([],[],pen=None,symbol='o')\n",
    "plot4 = win.addPlot()\n",
    "plot4.setXRange(-6,6)\n",
    "plot4.setYRange(0,6)\n",
    "plot4.setLabel('left',text = 'Y position (m)')\n",
    "plot4.setLabel('bottom', text= 'X position (m)')\n",
    "s4 = plot4.plot([],[],pen=None,symbol='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlvParsing(data, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes):\n",
    "    \n",
    "    data = np.frombuffer(data, dtype = 'uint8')\n",
    "    \n",
    "    targetDict = dict()\n",
    "    pointCloud = np.array([])\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index+4:index+8].view(dtype=np.uint32)\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    pointCloudDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType.size > 0 and tlvType == 6: #point cloud TLV\n",
    "        numberOfPoints = pointCloudDataLength/pointLengthInBytes\n",
    "#         print('NUMBER OF POINTS ', str(int(numberOfPoints)))\n",
    "        if numberOfPoints > 0:\n",
    "            p = data[index:index+pointCloudDataLength[0]].view(dtype=np.single)\n",
    "            #form the appropriate array \n",
    "            #each point is 16 bytes - 4 bytes for each property - range, azimuth, doppler, snr\n",
    "            pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "    \n",
    "    #increment the index so it is possible to read the target list\n",
    "    index += pointCloudDataLength\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index[0]:index[0]+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index[0]+4:index[0]+8].view(dtype=np.uint32)\n",
    "    index += tlvHeaderLengthInBytes\n",
    "    targetListDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType.size > 0 and tlvType == 7: #target List TLV\n",
    "        \n",
    "        numberOfTargets = targetListDataLength/targetLengthInBytes\n",
    "        TID = np.zeros((1, int(numberOfTargets[0])), dtype = np.uint32) #tracking IDs\n",
    "        kinematicData = np.zeros((6, int(numberOfTargets[0])), dtype = np.single)\n",
    "        errorCovariance = np.zeros((9, int(numberOfTargets[0])), dtype = np.single)\n",
    "        gatingGain = np.zeros((1, int(numberOfTargets[0])), dtype = np.single)\n",
    "        \n",
    "        #increment the index so it is possible to read the target list\n",
    "        targetIndex = 0\n",
    "        while targetIndex != int(numberOfTargets[0]):\n",
    "            TID[0][targetIndex] = data[index[0]:index[0]+4].view(dtype=np.uint32)\n",
    "            kinematicData[:,targetIndex] = data[index[0]+4:index[0]+28].view(dtype=np.single)\n",
    "            errorCovariance[:,targetIndex] = data[index[0]+28:index[0]+64].view(dtype=np.single)\n",
    "            gatingGain[:,targetIndex] = data[index[0]+64:index[0]+68].view(dtype=np.single)\n",
    "            index += targetLengthInBytes\n",
    "            targetIndex += 1\n",
    "            \n",
    "        targetDict['TID'] = TID\n",
    "        targetDict['kinematicData'] = kinematicData\n",
    "        targetDict['errorCovariance'] = errorCovariance\n",
    "        targetDict['gatingGain'] = gatingGain\n",
    "    \n",
    "    return pointCloud, targetDict\n",
    "\n",
    "def parsePointCloud(pointCloud): #remove points that are not within the boundary\n",
    "    \n",
    "    effectivePointCloud = np.array([])\n",
    "    \n",
    "    for index in range(0, len(pointCloud[0,:])):\n",
    "        if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "        and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "            and pointCloud[1, index] < 50*np.pi/180):\n",
    "\n",
    "            #concatenate columns to the new point cloud\n",
    "            if len(effectivePointCloud) == 0:\n",
    "                effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "            else:\n",
    "                point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "\n",
    "    if len(effectivePointCloud) != 0:\n",
    "        posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "        posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "        SNR  = effectivePointCloud[3,:]\n",
    "    \n",
    "        return posX,posY,SNR\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, parallel=True)\n",
    "def isinDFS(unvisitedNodes,visited):\n",
    "    #output is array the same size as unvisited nodes showing true/false for each element whether its in the array\n",
    "    unvisitedNodesMask = np.zeros(unvisitedNodes.size)\n",
    "    for i in nb.prange(0,unvisitedNodes.shape[0]):\n",
    "        mask = np.equal(unvisitedNodes[i], visited).any()\n",
    "        unvisitedNodesMask[i] = mask\n",
    "        \n",
    "    return unvisitedNodesMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def isinGeneral(testObj,collection):\n",
    "    #check whether testObj is in collection\n",
    "    for i in nb.prange(0, collection.shape[0]):\n",
    "        if np.equal(testObj, collection[i]):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True, parallel=True)\n",
    "def iterativeDfs(vertexID, edgeMatrix, startNode):\n",
    "    \n",
    "    visited = np.array([-1])\n",
    "    dfsStack = np.array([startNode])\n",
    "    \n",
    "    while dfsStack.size > 0:\n",
    "        vertex, dfsStack = dfsStack[-1], dfsStack[:-1] #equivalent to stack pop function\n",
    "        mask = isinGeneral(vertex,visited)\n",
    "        if np.logical_not(mask):\n",
    "            #find unvisited nodes\n",
    "            unvisitedNodes = vertexID[np.logical_not(isinDFS(edgeMatrix[int(vertex), :], np.array([np.inf])))]\n",
    "            if (np.equal(visited,np.array([-1]))).any():\n",
    "                visited = np.array([vertex], dtype=np.int64)\n",
    "            else:\n",
    "                visited = np.concatenate((visited, np.array([vertex])))\n",
    "            #add unvisited nodes to the stack\n",
    "            dfsStack = np.unique(np.concatenate((dfsStack, unvisitedNodes[np.logical_not(isinDFS(unvisitedNodes,visited))])))\n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterativeDfsOriginal(vertexID, edgeMatrix, startNode):\n",
    "    \n",
    "    visited = np.array([], dtype=np.int)\n",
    "    dfsStack = np.array([startNode])\n",
    "\n",
    "    while dfsStack.size > 0:\n",
    "        vertex, dfsStack = dfsStack[-1], dfsStack[:-1] #equivalent to stack pop function\n",
    "        if vertex not in visited:\n",
    "            #find unvisited nodes\n",
    "            unvisitedNodes = vertexID[np.logical_not(np.isnan(edgeMatrix[int(vertex), :]))]\n",
    "            visited = np.append(visited, vertex)\n",
    "            #add unvisited nodes to the stack\n",
    "            dfsStack = np.append(dfsStack, unvisitedNodes[np.logical_not(np.isin(unvisitedNodes,visited))])\n",
    "    \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(parallel=True)\n",
    "def distanceMatrix(posX, posY, weightThreshold):\n",
    "    \n",
    "    edgeMatrix = np.zeros((len(posX), len(posY)), dtype=np.float64)\n",
    "    \n",
    "    #create distance matrix\n",
    "    #x1 - x0\n",
    "    xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                              np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "    #y1 - y0\n",
    "    yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                              np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "    #euclidean distance calculation\n",
    "    edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "    #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "    weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "    edgeMatrix[weightMask] = np.inf\n",
    "    \n",
    "    return edgeMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreeClustering(posX, posY, SNR, weightThreshold, minClusterSize):\n",
    "    \n",
    "    vertexID = np.arange(len(posX))\n",
    "    vertexList = np.arange(len(posX))\n",
    "\n",
    "    associatedPoints = np.array([])\n",
    "\n",
    "    if len(posX) >= minClusterSize:\n",
    "        edgeMatrix = distanceMatrix(posX, posY, weightThreshold)\n",
    "\n",
    "        #perform iterative dfs\n",
    "        associatedPoints = np.array([])\n",
    "        \n",
    "        \n",
    "        centroidNumber = 0\n",
    "        while vertexID.size > 0:\n",
    "            startNode = vertexID[0]\n",
    "            visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "            #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "            vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "#             #visited is a component, extract cluster from it if possible\n",
    "            if visited.size >= minClusterSize:\n",
    "                cluster =  np.array([posX[visited], posY[visited],SNR[visited],\n",
    "                                     np.repeat(centroidNumber, repeats=len(visited))])\n",
    "                if associatedPoints.size == 0:\n",
    "                    associatedPoints = cluster\n",
    "                else:\n",
    "                    associatedPoints = np.hstack((associatedPoints, cluster))\n",
    "                centroidNumber += 1\n",
    "                                    \n",
    "                \n",
    "\n",
    "\n",
    "    return associatedPoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, P, A, Q): #predict function\n",
    "    xpred = np.matmul(A,x)\n",
    "    Ppred = np.matmul(A,P)\n",
    "    Ppred = np.matmul(Ppred,np.transpose(A)) + Q\n",
    "    return(xpred, Ppred)\n",
    "\n",
    "def innovation(xpred, Ppred, z, H, R): #innovation function\n",
    "    nu = z - np.matmul(H,xpred)\n",
    "    S = np.matmul(H,Ppred)\n",
    "    S = R + np.matmul(S, np.transpose(H))\n",
    "    return(nu, S)\n",
    "\n",
    "def innovation_update(xpred, Ppred, nu, S, H):\n",
    "    K = np.matmul(Ppred, np.transpose(H))\n",
    "    K = np.matmul(K,np.linalg.inv(S)) #check inverse function\n",
    "    xnew = xpred + np.matmul(K,nu)\n",
    "    Pnew = np.matmul(K,S)\n",
    "    Pnew = Ppred - np.matmul(Pnew,np.transpose(K)) \n",
    "    return(xnew, Pnew)\n",
    "\n",
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return(rho, phi)\n",
    "\n",
    "def data_associate(centroidPred, rthetacentroid):\n",
    "    rthetacentroidCurrent = rthetacentroid\n",
    "    centpredCol = np.size(centroidPred,1)\n",
    "    rthetaCol = np.size(rthetacentroid,1)\n",
    "\n",
    "    for i in list(range(0,centpredCol)):\n",
    "        r1 = centroidPred[0][i]\n",
    "        r2 = rthetacentroid[0]\n",
    "        theta1 = centroidPred[2][i]\n",
    "        theta2 = rthetacentroid[1]\n",
    "        temp = np.sqrt(np.multiply(r1,r1) + np.multiply(r2,r2) - np.multiply(np.multiply(np.multiply(2,r1),r2),np.cos(theta2-theta1)))\n",
    "        if(i==0):\n",
    "            minDist = temp\n",
    "        else:\n",
    "            minDist = np.vstack((minDist,temp))\n",
    "\n",
    "    currentFrame = np.empty((2,max(centpredCol,rthetaCol)))\n",
    "    currentFrame[:] = np.nan\n",
    "\n",
    "    minDist = np.reshape(minDist, (centpredCol,rthetaCol))\n",
    "    minDistOrg = minDist\n",
    "\n",
    "    for i in list(range(0,min(centpredCol,rthetaCol))):\n",
    "        if((np.ndim(minDist)) == 1):\n",
    "            minDist = np.reshape(minDist,(rthetaCol,1))\n",
    "            minDistOrg = np.reshape(minDistOrg,(rthetaCol,1))\n",
    "        val = np.min(minDist)\n",
    "        resultOrg = np.argwhere(minDistOrg == val)\n",
    "        result = np.argwhere(minDist == val)\n",
    "        minRowOrg = resultOrg[0][0]\n",
    "        minColOrg = resultOrg[0][1]\n",
    "        minRow = result[0][0]\n",
    "        minCol = result[0][1]\n",
    "        currentFrame[:,minRowOrg] = rthetacentroid[:,minColOrg]\n",
    "        minDist = np.delete(minDist,minRow,0)\n",
    "        minDist = np.delete(minDist,minCol,1)\n",
    "        rthetacentroidCurrent = np.delete(rthetacentroidCurrent,minCol,1)\n",
    "\n",
    "    index = 0\n",
    "    if (rthetacentroidCurrent.size != 0): #check indexing\n",
    "        for i in list(range(centpredCol,rthetaCol)):\n",
    "            currentFrame[:,i] = rthetacentroidCurrent[:,index]\n",
    "            index += 1 \n",
    "\n",
    "    return(currentFrame)\n",
    "\n",
    "def LiveRKF(currentrawxycentroidData, centroidX, centroidP, Q, R, isFirst):\n",
    "    \n",
    "    \n",
    "    #initialise matrices \n",
    "    delT = 0.0500\n",
    "    A = np.array([[1,delT,0,0], \n",
    "                  [0,1  ,0,0], \n",
    "                  [0,0,1,delT], \n",
    "                  [0,0,0,1]])\n",
    "    H = np.array([[1,0,0,0],\n",
    "                  [0,0,1,0]])\n",
    "    P = np.identity(4)\n",
    "\n",
    "    xytransposecentroidData = currentrawxycentroidData\n",
    "    rthetacentroidData=xytransposecentroidData\n",
    "    if (xytransposecentroidData.size != 0): \n",
    "        [rthetacentroidData[0,:],rthetacentroidData[1,:]] = cart2pol(xytransposecentroidData[0,:],xytransposecentroidData[1,:])\n",
    "    if(isFirst):\n",
    "        centroidX[[0,2],0] = rthetacentroidData[[0,1],0]\n",
    "        isFirst = 0\n",
    "    if((rthetacentroidData.size != 0)):\n",
    "        currentFrame = data_associate(centroidX, rthetacentroidData)\n",
    "        addittionalCentroids = (np.size(rthetacentroidData,1)-np.size(centroidX,1))\n",
    "        if(addittionalCentroids>0):\n",
    "            truncateCurrentFrame = currentFrame[:,np.size(centroidX,1):np.size(currentFrame,1)]\n",
    "            zeroTemplate = np.zeros((4,np.size(truncateCurrentFrame,1)),dtype=truncateCurrentFrame.dtype)\n",
    "            zeroTemplate[[0,2],:] = truncateCurrentFrame[[0,1],:]\n",
    "            centroidX = np.hstack((centroidX,zeroTemplate))\n",
    "            for newFrameIndex in list((range(0, addittionalCentroids))):\n",
    "                centroidP.extend([P])\n",
    "        for currentFrameIndex in list((range(0,np.size(currentFrame,1)))):\n",
    "            if(not(np.isnan(currentFrame[0,currentFrameIndex]))):\n",
    "                [xpred, Ppred] = predict(centroidX[:,currentFrameIndex], centroidP[currentFrameIndex], A, Q)\n",
    "                [nu, S] = innovation(xpred, Ppred, currentFrame[:, currentFrameIndex], H, R)\n",
    "                [centroidX[:,currentFrameIndex],  centroidP[currentFrameIndex]] = innovation_update(xpred, Ppred, nu, S, H)\n",
    "            else:\n",
    "                [centroidX[:,currentFrameIndex], centroidP[currentFrameIndex]] = predict(centroidX[:,currentFrameIndex], centroidP[currentFrameIndex], A, Q)                   \n",
    "    else:\n",
    "        for noFrameIndex in list((range(0,np.size(centroidX,1)))):\n",
    "            [centroidX[:,noFrameIndex], centroidP[noFrameIndex]] = predict(centroidX[:,noFrameIndex], centroidP[noFrameIndex], A, Q)\n",
    "    #centroidX is 4xN array that contains that centroid information for that frame\n",
    "    return centroidX, centroidP,isFirst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mInvalid use of Function(<built-in function setitem>) with argument(s) of type(s): (array(float32, 2d, C), array(bool, 2d, C), float64)\n * parameterized\n\u001b[1mIn definition 0:\u001b[0m\n\u001b[1m    All templates rejected with literals.\u001b[0m\n\u001b[1mIn definition 1:\u001b[0m\n\u001b[1m    All templates rejected without literals.\u001b[0m\n\u001b[1mIn definition 2:\u001b[0m\n\u001b[1m    All templates rejected with literals.\u001b[0m\n\u001b[1mIn definition 3:\u001b[0m\n\u001b[1m    All templates rejected without literals.\u001b[0m\n\u001b[1mIn definition 4:\u001b[0m\n\u001b[1m    All templates rejected with literals.\u001b[0m\n\u001b[1mIn definition 5:\u001b[0m\n\u001b[1m    All templates rejected without literals.\u001b[0m\n\u001b[1mIn definition 6:\u001b[0m\n\u001b[1m    TypeError: unsupported array index type array(bool, 2d, C) in [array(bool, 2d, C)]\u001b[0m\n    raised from C:\\Users\\hasna\\Anaconda3\\lib\\site-packages\\numba\\typing\\arraydecl.py:71\n\u001b[1mIn definition 7:\u001b[0m\n\u001b[1m    TypeError: unsupported array index type array(bool, 2d, C) in [array(bool, 2d, C)]\u001b[0m\n    raised from C:\\Users\\hasna\\Anaconda3\\lib\\site-packages\\numba\\typing\\arraydecl.py:71\n\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\n\u001b[0m\u001b[1m[1] During: typing of setitem at <ipython-input-65-257c89911810> (18)\u001b[0m\n\u001b[1m\nFile \"<ipython-input-65-257c89911810>\", line 18:\u001b[0m\n\u001b[1mdef distanceMatrix(posX, posY, weightThreshold):\n    <source elided>\n    weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n\u001b[1m    edgeMatrix[weightMask] = np.inf\n\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/dev/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/dev/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-9532533ef1ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m#initial noise reduction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         clusters = TreeClustering(posX, posY, SNR,\n\u001b[1;32m---> 60\u001b[1;33m                                       weightThresholdIntial, minClusterSizeInitial)\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-a032c0fd7c7b>\u001b[0m in \u001b[0;36mTreeClustering\u001b[1;34m(posX, posY, SNR, weightThreshold, minClusterSize)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mminClusterSize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0medgeMatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistanceMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweightThreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m#perform iterative dfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numba\\dispatcher.py\u001b[0m in \u001b[0;36m_compile_for_args\u001b[1;34m(self, *args, **kws)\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m             \u001b[0merror_rewrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'typing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;31m# Something unsupported is present in the user code, add help info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numba\\dispatcher.py\u001b[0m in \u001b[0;36merror_rewrite\u001b[1;34m(e, issue_type)\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m                 \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0margtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numba\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1mInvalid use of Function(<built-in function setitem>) with argument(s) of type(s): (array(float32, 2d, C), array(bool, 2d, C), float64)\n * parameterized\n\u001b[1mIn definition 0:\u001b[0m\n\u001b[1m    All templates rejected with literals.\u001b[0m\n\u001b[1mIn definition 1:\u001b[0m\n\u001b[1m    All templates rejected without literals.\u001b[0m\n\u001b[1mIn definition 2:\u001b[0m\n\u001b[1m    All templates rejected with literals.\u001b[0m\n\u001b[1mIn definition 3:\u001b[0m\n\u001b[1m    All templates rejected without literals.\u001b[0m\n\u001b[1mIn definition 4:\u001b[0m\n\u001b[1m    All templates rejected with literals.\u001b[0m\n\u001b[1mIn definition 5:\u001b[0m\n\u001b[1m    All templates rejected without literals.\u001b[0m\n\u001b[1mIn definition 6:\u001b[0m\n\u001b[1m    TypeError: unsupported array index type array(bool, 2d, C) in [array(bool, 2d, C)]\u001b[0m\n    raised from C:\\Users\\hasna\\Anaconda3\\lib\\site-packages\\numba\\typing\\arraydecl.py:71\n\u001b[1mIn definition 7:\u001b[0m\n\u001b[1m    TypeError: unsupported array index type array(bool, 2d, C) in [array(bool, 2d, C)]\u001b[0m\n    raised from C:\\Users\\hasna\\Anaconda3\\lib\\site-packages\\numba\\typing\\arraydecl.py:71\n\u001b[1mThis error is usually caused by passing an argument of a type that is unsupported by the named function.\u001b[0m\u001b[0m\n\u001b[0m\u001b[1m[1] During: typing of setitem at <ipython-input-65-257c89911810> (18)\u001b[0m\n\u001b[1m\nFile \"<ipython-input-65-257c89911810>\", line 18:\u001b[0m\n\u001b[1mdef distanceMatrix(posX, posY, weightThreshold):\n    <source elided>\n    weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n\u001b[1m    edgeMatrix[weightMask] = np.inf\n\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n\nThis is not usually a problem with Numba itself but instead often caused by\nthe use of unsupported features or an issue in resolving types.\n\nTo see Python/NumPy features supported by the latest release of Numba visit:\nhttp://numba.pydata.org/numba-doc/dev/reference/pysupported.html\nand\nhttp://numba.pydata.org/numba-doc/dev/reference/numpysupported.html\n\nFor more information about typing errors and how to debug them visit:\nhttp://numba.pydata.org/numba-doc/latest/user/troubleshoot.html#my-code-doesn-t-compile\n\nIf you think your code should work with Numba, please report the error message\nand traceback, along with a minimal reproducer at:\nhttps://github.com/numba/numba/issues/new\n"
     ]
    }
   ],
   "source": [
    "parsingMatFile = 'C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Experiment Data 2\\\\3PeopleWalking.mat'\n",
    "# parsingMatFile = 'C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\fastWalk.mat'\n",
    "\n",
    "tlvData = (loadmat(parsingMatFile))['tlvStream'][0]\n",
    "\n",
    "#RKF \n",
    "centroidX =np.zeros((4,1))\n",
    "centroidP = []\n",
    "P = np.identity(4);\n",
    "centroidP.extend([P])\n",
    "\n",
    "#covariance matrices\n",
    "Q = np.eye(4)\n",
    "R = np.ones(2).reshape(-1,1)\n",
    "\n",
    "#tree based\n",
    "weightThresholdIntial = 0.2 #minimum distance between points\n",
    "minClusterSizeInitial = 10\n",
    "weightThresholdFinal = 0.8 #minimum distance between points\n",
    "minClusterSizeFinal = 8 \n",
    "\n",
    "\n",
    "#zone snr\n",
    "snrFirstZone = 10\n",
    "snrMiddleZone = 15\n",
    "snrLastZone = 5\n",
    "\n",
    "tlvHeaderLengthInBytes = 8\n",
    "pointLengthInBytes = 16\n",
    "targetLengthInBytes = 68\n",
    "\n",
    "#optimization\n",
    "expectedClustering = np.array([])\n",
    "expectedKalman = np.array([])\n",
    "\n",
    "s1.setData([],[])\n",
    "s2.setData([],[])\n",
    "s3.setData([],[])\n",
    "s4.setData([],[])\n",
    "QtGui.QApplication.processEvents()\n",
    "\n",
    "tiPosX = np.array([])\n",
    "tiPosY = np.array([])\n",
    "\n",
    "Q = np.multiply(0.2,np.identity(4))\n",
    "R = np.multiply(5,np.array([[1],[1]]))\n",
    "\n",
    "isFirst = 1\n",
    "\n",
    "for tlvStream in tlvData:\n",
    "    \n",
    "    #parsing\n",
    "    pointCloud, targetDict = tlvParsing(tlvStream, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes)\n",
    "    \n",
    "    if pointCloud.size > 0:\n",
    "        posX,posY,SNR = parsePointCloud(pointCloud) #dictionary that contains the point cloud data\n",
    "        s1.setData(posX, posY)\n",
    "        #initial noise reduction\n",
    "        clusters = TreeClustering(posX, posY, SNR,\n",
    "                                      weightThresholdIntial, minClusterSizeInitial)\n",
    "        \n",
    "        if clusters.size > 0:\n",
    "            \n",
    "            \n",
    "#             row 1 - x\n",
    "#             row 2 - y\n",
    "#             row 3 - SNR\n",
    "#             row 4 - cluster number\n",
    "            \n",
    "#             snr zone snr test\n",
    "#             4.5 to the end -> last zone\n",
    "#             3-4.5m -> middle zone\n",
    "#             1-3m -> first zone\n",
    "            snrMask_LastZone = np.logical_and(np.greater(clusters[1,:], 4.5), np.greater(clusters[2,:], snrLastZone)) #zone 4.5m and greater\n",
    "            snrMask_MiddleZone = np.logical_and(np.logical_and(np.greater(clusters[1,:], 3), np.less_equal(clusters[1,:], 4.5)), \n",
    "                                                np.greater(clusters[2,:], snrMiddleZone)) #zone 3-4.5m with SNR > 20\n",
    "            snrMask_FirstZone = np.logical_and(np.less_equal(clusters[1,:], 3), np.greater(clusters[2,:], snrFirstZone))\n",
    "            overallSnrMask = np.logical_or(np.logical_or(snrMask_FirstZone,snrMask_MiddleZone), snrMask_LastZone)\n",
    "\n",
    "            snrFilteredClusters = clusters[:,overallSnrMask]\n",
    "\n",
    "            if snrFilteredClusters.size > 0:\n",
    "#                 s2.setData(snrFilteredClusters[0,:], snrFilteredClusters[1,:])\n",
    "                \n",
    "                dbClusters = TreeClustering(snrFilteredClusters[0,:], snrFilteredClusters[1,:], \n",
    "                                                snrFilteredClusters[2,:], \n",
    "                                                weightThresholdFinal, minClusterSizeFinal)\n",
    "                if dbClusters.size > 0:\n",
    "                    #row 1 - x\n",
    "                    #row 2 - y\n",
    "                    #row 3 - cluster number\n",
    "                    k = int(max(dbClusters[3,:])) + 1 \n",
    "                    points = np.transpose(np.array([dbClusters[0,:], dbClusters[1,:]]))\n",
    "                  \n",
    "                    #kmeans \n",
    "                    centroidClusterer = KMeans(n_clusters= k).fit(points)\n",
    "                    s2.setData(centroidClusterer.cluster_centers_[:,0], centroidClusterer.cluster_centers_[:,1])\n",
    "                    centroidData = np.array([centroidClusterer.cluster_centers_[:,0], centroidClusterer.cluster_centers_[:,1]])\n",
    "\n",
    "                    #tracking\n",
    "                    centroidX, centroidP,isFirst = LiveRKF(centroidData, centroidX, centroidP, Q, R, isFirst)\n",
    "                    #plot\n",
    "                    #calculate x and y positions\n",
    "                    xPositions = np.multiply(centroidX[0,:], np.cos(centroidX[2,:]))\n",
    "                    yPositions = np.multiply(centroidX[0,:], np.sin(centroidX[2,:]))\n",
    "                   \n",
    "                    #calculate range and theta to remove from graphing\n",
    "#                     r = np.sqrt(np.add(np.square(xPositions),np.square(yPositions)))\n",
    "#                     theta = np.rad2deg(np.arctan(np.divide(yPositions,xPositions)))\n",
    "#                     positionMask = np.logical_and(np.less_equal(r,6), np.logical_and(np.less_equal(theta, 50), np.greater_equal(theta, -50)))\n",
    "                    \n",
    "#                     xPositions = xPositions[positionMask]\n",
    "#                     yPositions = yPositions[positionMask]\n",
    "                    \n",
    "                    s3.setData(xPositions, yPositions)\n",
    "\n",
    "    \n",
    "    if len(targetDict) != 0:\n",
    "        #kinematic data object structure\n",
    "        #row 0 - posX\n",
    "        #row 1 - posY \n",
    "        #row 2 - velX\n",
    "        #row 3 - velY\n",
    "        #row 4 - accX\n",
    "        #row 5 - accY\n",
    "        tiPosX = targetDict['kinematicData'][0,:]\n",
    "        tiPosY = targetDict['kinematicData'][1,:]\n",
    "        s4.setData(tiPosX,tiPosY)\n",
    "        \n",
    "    QtGui.QApplication.processEvents()\n",
    "#     k = input('ENTER ')\n",
    "#     time.sleep(0.05)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.08725739, -0.94363821, -1.01036   , -0.84362501, -1.00082839,\n",
       "        -0.93410647, -1.09835196],\n",
       "       [ 4.66299248,  4.74398947,  5.07942247,  5.10976124,  5.03150368,\n",
       "         4.69607019,  4.71057463],\n",
       "       [13.21271992, 30.99615097, 12.68980885, 12.71436501, 10.13994694,\n",
       "        12.09743977, 27.89948463],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snrFilteredClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
