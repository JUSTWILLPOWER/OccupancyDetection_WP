{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import cProfile\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "import pyqtgraph as pg\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from os import listdir\n",
    "import seaborn as sns \n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlvParsing(data, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes):\n",
    "    \n",
    "    data = np.frombuffer(data, dtype = 'uint8')\n",
    "    \n",
    "    targetDict = dict()\n",
    "    pointCloud = np.array([])\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index+4:index+8].view(dtype=np.uint32)\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    pointCloudDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType.size > 0 and tlvType == 6: #point cloud TLV\n",
    "        numberOfPoints = pointCloudDataLength/pointLengthInBytes\n",
    "        if numberOfPoints > 0:\n",
    "            p = data[index:index+pointCloudDataLength[0]].view(dtype=np.single)\n",
    "            #form the appropriate array \n",
    "            #each point is 16 bytes - 4 bytes for each property - range, azimuth, doppler, snr\n",
    "            pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "    \n",
    "    #increment the index so it is possible to read the target list\n",
    "    index += pointCloudDataLength\n",
    "    #tlv header parsing\n",
    "    tlvType = data[index[0]:index[0]+4].view(dtype=np.uint32)\n",
    "    tlvLength = data[index[0]+4:index[0]+8].view(dtype=np.uint32)\n",
    "    index += tlvHeaderLengthInBytes\n",
    "    targetListDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "    if tlvType.size > 0 and tlvType == 7: #target List TLV\n",
    "        \n",
    "        numberOfTargets = targetListDataLength/targetLengthInBytes\n",
    "        TID = np.zeros((1, int(numberOfTargets[0])), dtype = np.uint32) #tracking IDs\n",
    "        kinematicData = np.zeros((6, int(numberOfTargets[0])), dtype = np.single)\n",
    "        errorCovariance = np.zeros((9, int(numberOfTargets[0])), dtype = np.single)\n",
    "        gatingGain = np.zeros((1, int(numberOfTargets[0])), dtype = np.single)\n",
    "        \n",
    "        #increment the index so it is possible to read the target list\n",
    "        targetIndex = 0\n",
    "        while targetIndex != int(numberOfTargets[0]):\n",
    "            TID[0][targetIndex] = data[index[0]:index[0]+4].view(dtype=np.uint32)\n",
    "            kinematicData[:,targetIndex] = data[index[0]+4:index[0]+28].view(dtype=np.single)\n",
    "            errorCovariance[:,targetIndex] = data[index[0]+28:index[0]+64].view(dtype=np.single)\n",
    "            gatingGain[:,targetIndex] = data[index[0]+64:index[0]+68].view(dtype=np.single)\n",
    "            index += targetLengthInBytes\n",
    "            targetIndex += 1\n",
    "            \n",
    "        targetDict['TID'] = TID\n",
    "        targetDict['kinematicData'] = kinematicData\n",
    "        targetDict['errorCovariance'] = errorCovariance\n",
    "        targetDict['gatingGain'] = gatingGain\n",
    "    \n",
    "    return pointCloud, targetDict\n",
    "\n",
    "def parsePointCloud(pointCloud): #remove points that are not within the boundary\n",
    "    \n",
    "    effectivePointCloud = np.array([])\n",
    "    posX = np.array([])\n",
    "    posY = np.array([])\n",
    "    SNR = np.array([])\n",
    "    \n",
    "    for index in range(0, len(pointCloud[0,:])):\n",
    "        if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "        and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "            and pointCloud[1, index] < 50*np.pi/180):\n",
    "\n",
    "            #concatenate columns to the new point cloud\n",
    "            if len(effectivePointCloud) == 0:\n",
    "                effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "            else:\n",
    "                point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "\n",
    "    if len(effectivePointCloud) != 0:\n",
    "        posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "        posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "        SNR  = effectivePointCloud[3,:]\n",
    "    \n",
    "    return posX,posY,SNR\n",
    "\n",
    "def iterativeDfs(vertexID, edgeMatrix, startNode):\n",
    "    \n",
    "    visited = np.array([], dtype=np.int)\n",
    "    dfsStack = np.array([startNode])\n",
    "\n",
    "    while dfsStack.size > 0:\n",
    "        vertex, dfsStack = dfsStack[-1], dfsStack[:-1] #equivalent to stack pop function\n",
    "        if vertex not in visited:\n",
    "            #find unvisited nodes\n",
    "            unvisitedNodes = vertexID[np.logical_not(np.isnan(edgeMatrix[int(vertex), :]))]\n",
    "            visited = np.append(visited, vertex)\n",
    "            #add unvisited nodes to the stack\n",
    "            dfsStack = np.append(dfsStack, unvisitedNodes[np.logical_not(np.isin(unvisitedNodes,visited))])\n",
    "    \n",
    "    return visited\n",
    "\n",
    "def TreeClustering(posX, posY, SNR, weightThreshold, minClusterSize):\n",
    "    \n",
    "    vertexID = np.arange(len(posX))\n",
    "    vertexList = np.arange(len(posX))\n",
    "\n",
    "    associatedPoints = np.array([])\n",
    "\n",
    "    if len(posX) >= minClusterSize:\n",
    "        edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "\n",
    "        #create distance matrix\n",
    "        #x1 - x0\n",
    "        xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                                  np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "        #y1 - y0\n",
    "        yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                                  np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "        #euclidean distance calculation\n",
    "        edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "        #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "        weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "        edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "        #perform iterative dfs\n",
    "        associatedPoints = np.array([])\n",
    "        \n",
    "        centroidNumber = 0\n",
    "        while vertexID.size > 0:\n",
    "            startNode = vertexID[0]\n",
    "            visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "            #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "            vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "#             #visited is a component, extract cluster from it if possible\n",
    "            if visited.size >= minClusterSize:\n",
    "                cluster =  np.array([posX[visited], posY[visited],SNR[visited],\n",
    "                                     np.repeat(centroidNumber, repeats=len(visited))])\n",
    "                if associatedPoints.size == 0:\n",
    "                    associatedPoints = cluster\n",
    "                else:\n",
    "                    associatedPoints = np.hstack((associatedPoints, cluster))\n",
    "                centroidNumber += 1\n",
    "\n",
    "    return associatedPoints\n",
    "\n",
    "#Functions adapted from Ian Reid's Estimation II: Discrete Kalman Filter (In Compendium)\n",
    "\n",
    "def kalmanPredictionStep(stateVariables, covarianceMatrix, systemMatrix, systemCovariance): #predict function\n",
    "    predictionState = np.matmul(systemMatrix,stateVariables) #Predict usng system matrix and system variables\n",
    "    predictionCovariance = np.matmul(systemMatrix,covarianceMatrix) #Error covariance prediction\n",
    "    predictionCovariance = np.matmul(predictionCovariance,np.transpose(systemMatrix)) + systemCovariance\n",
    "    return(predictionState, predictionCovariance)\n",
    "\n",
    "\n",
    "def kalmanInnovationStep(predictionState, predictionCovariance, newMeasurement, outputMatrix, measurementCovariance): #innovation function (splits update function in two essentially)\n",
    "    innovationDifference = newMeasurement - np.matmul(outputMatrix,predictionState) #difference between measured and prediction\n",
    "    innovationOutput = np.matmul(outputMatrix,predictionCovariance) #innovation covariance computation\n",
    "    innovationOutput = measurementCovariance + np.matmul(innovationOutput, np.transpose(outputMatrix))\n",
    "    return(innovationDifference, innovationOutput)\n",
    "\n",
    "def kalmanInnovationUpdate(predictionState, predictionCovariance, innovationDifference,innovationOutput, outputMatrix): #kalman update funciton\n",
    "    kalmanGain = np.matmul(predictionCovariance, np.transpose(outputMatrix))\n",
    "    kalmanGain = np.matmul(kalmanGain,np.linalg.inv(innovationOutput)) #Recurisve computation of new kalman gain\n",
    "    newStatePrediction = predictionState + np.matmul(kalmanGain,innovationDifference)\n",
    "    newPredictionCovariance = np.matmul(kalmanGain,innovationOutput) #Calculate new error covairance matrix\n",
    "    newPredictionCovariance = predictionCovariance - np.matmul(newPredictionCovariance,np.transpose(kalmanGain)) \n",
    "    return(newStatePrediction, newPredictionCovariance)\n",
    "\n",
    "def cart2pol(x, y): #converts cartesian to polar cooridnates \n",
    "    rho = np.sqrt(x**2 + y**2) #radial component\n",
    "    phi = np.arctan2(y, x)#theta component\n",
    "    return(rho, phi)\n",
    "\n",
    "def data_associate(centroidPred, rthetacentroid): #inputs: new measurement and previous measurement\n",
    "    minDist = np.array([]) #initialise temp arrays\n",
    "    rthetacentroidCurrent = rthetacentroid\n",
    "    centpredCol = np.size(centroidPred,1)\n",
    "    rthetaCol = np.size(rthetacentroid,1)\n",
    "\n",
    "    for i in list(range(0,centpredCol)):\n",
    "        r1 = centroidPred[0][i] #extract preivous radial measurement for each centroid per loop\n",
    "        r2 = rthetacentroid[0]#extract all new radial measurements\n",
    "        theta1 = centroidPred[2][i] \n",
    "        theta2 = rthetacentroid[1]\n",
    "        #calculate euclidian distance between each previous measurement and all new measurements\n",
    "        temp = np.sqrt(np.multiply(r1,r1) + np.multiply(r2,r2) - np.multiply(np.multiply(np.multiply(2,r1),r2),np.cos(theta2-theta1)))\n",
    "        if(i==0):\n",
    "            minDist = temp\n",
    "        else:\n",
    "            minDist = np.vstack((minDist,temp)) #store distance matrix \n",
    "\n",
    "    currentFrame = np.empty((2,max(centpredCol,rthetaCol))) #initialise frame for current frame's centroids\n",
    "    currentFrame[:] = np.nan\n",
    "\n",
    "    minDist = np.reshape(minDist, (centpredCol,rthetaCol))\n",
    "    minDistOrg = minDist #store distance matrix in an array for reference as minDist will be modified as associated\n",
    "\n",
    "    for i in list(range(0,min(centpredCol,rthetaCol))): #loop through the minimum number of centroids using GNN approach\n",
    "        if((np.ndim(minDist)) == 1): \n",
    "            minDist = np.reshape(minDist,(rthetaCol,1))\n",
    "            minDistOrg = np.reshape(minDistOrg,(rthetaCol,1))\n",
    "        val = np.min(minDist) #extract smallest distance\n",
    "        resultOrg = np.argwhere(minDistOrg == val) #find original indicies of minimum distance\n",
    "        result = np.argwhere(minDist == val) #find new indicies of minimum distance in minDist\n",
    "        minRowOrg = resultOrg[0][0] #extract original and new distance matrix indicies\n",
    "        minColOrg = resultOrg[0][1]\n",
    "        minRow = result[0][0]\n",
    "        minCol = result[0][1]\n",
    "        currentFrame[:,minRowOrg] = rthetacentroid[:,minColOrg] #extract centroid associated with minimum distnace\n",
    "        minDist = np.delete(minDist,minRow,0) #delete from the modified minimum distance so it is not associated again\n",
    "        minDist = np.delete(minDist,minCol,1)\n",
    "        rthetacentroidCurrent = np.delete(rthetacentroidCurrent,minCol,1) \n",
    "\n",
    "    index = 0\n",
    "    if (rthetacentroidCurrent.size != 0): #Check if centroids left unassociated\n",
    "        for i in list(range(centpredCol,rthetaCol)):\n",
    "            currentFrame[:,i] = rthetacentroidCurrent[:,index] #Add to new centriods (unnasociated)\n",
    "            index += 1 \n",
    "\n",
    "    return(currentFrame)\n",
    "\n",
    "def LiveRKF(currentrawxycentroidData, centroidX, centroidP, Q, R, isFirst):\n",
    "    #centroidX is 4xN array that contains that centroid information for that frame\n",
    "    #currentrawxycentroidData:new measured data\n",
    "    #centroidP : error covariance amtrix\n",
    "    \n",
    "    #initialise matrices \n",
    "    delT = 0.0500 \n",
    "    A = np.array([[1,delT,0,0], \n",
    "                  [0,1  ,0,0], \n",
    "                  [0,0,1,delT], \n",
    "                  [0,0,0,1]])\n",
    "    H = np.array([[1,0,0,0],\n",
    "                  [0,0,1,0]])\n",
    "    P = np.identity(4)\n",
    "\n",
    "    xytransposecentroidData = currentrawxycentroidData\n",
    "    rthetacentroidData=xytransposecentroidData\n",
    "    if (xytransposecentroidData.size != 0):  #convert from cartesian to polar coordinates \n",
    "        [rthetacentroidData[0,:],rthetacentroidData[1,:]] = cart2pol(xytransposecentroidData[0,:],xytransposecentroidData[1,:])\n",
    "    if(isFirst): #Initialise first centroid at its measured location\n",
    "        centroidX[[0,2],0] = rthetacentroidData[[0,1],0]\n",
    "        isFirst = 0 #set boolean to false \n",
    "    if((rthetacentroidData.size != 0)): #if there are meausred centroids in current frame\n",
    "        currentFrame = data_associate(centroidX, rthetacentroidData) #Data Association performed\n",
    "        addittionalCentroids = (np.size(rthetacentroidData,1)-np.size(centroidX,1)) #How many new centroids/occupants\n",
    "        if(addittionalCentroids>0): #If new centroids: Create new matrices/columns in centriods matrix, covariance matrix etc\n",
    "            truncateCurrentFrame = currentFrame[:,np.size(centroidX,1):np.size(currentFrame,1)]\n",
    "            zeroTemplate = np.zeros((4,np.size(truncateCurrentFrame,1)),dtype=truncateCurrentFrame.dtype)\n",
    "            zeroTemplate[[0,2],:] = truncateCurrentFrame[[0,1],:]\n",
    "            centroidX = np.hstack((centroidX,zeroTemplate)) #create new column for new centroids\n",
    "            for newFrameIndex in list((range(0, addittionalCentroids))):\n",
    "                centroidP.extend([P]) #create new covariance matrix\n",
    "        for currentFrameIndex in list((range(0,np.size(currentFrame,1)))): #loop through current frame of centroids\n",
    "            if(not(np.isnan(currentFrame[0,currentFrameIndex]))): #if not empty\n",
    "                #step1: Kalman prediction\n",
    "                [predictionState, predictionCovariance] = kalmanPredictionStep(centroidX[:,currentFrameIndex], centroidP[currentFrameIndex], A, Q)\n",
    "                #Kalman innovation\n",
    "                [innovationDifference, innovationOutput] = kalmanInnovationStep(predictionState, predictionCovariance, currentFrame[:, currentFrameIndex], H, R)\n",
    "                #Kalman update \n",
    "                [centroidX[:,currentFrameIndex],  centroidP[currentFrameIndex]] = kalmanInnovationUpdate(predictionState, predictionCovariance, innovationDifference, innovationOutput, H)\n",
    "            else: #if new meausred frame has no data\n",
    "                #predict using preious measurements\n",
    "                [centroidX[:,currentFrameIndex], centroidP[currentFrameIndex]] = kalmanPredictionStep(centroidX[:,currentFrameIndex], centroidP[currentFrameIndex], A, Q)                   \n",
    "    else:#if new measured frame has no data\n",
    "        for noFrameIndex in list((range(0,np.size(centroidX,1)))):\n",
    "            #Only kalman predict step\n",
    "            [centroidX[:,noFrameIndex], centroidP[noFrameIndex]] = kalmanPredictionStep(centroidX[:,noFrameIndex], centroidP[noFrameIndex], A, Q)\n",
    "\n",
    "    return centroidX, centroidP,isFirst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up plottig GUI\n",
    "app = QtGui.QApplication([])\n",
    "pg.setConfigOption('background','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the plot\n",
    "pg.setConfigOption('background', 'w')\n",
    "pg.setConfigOption('foreground', 'k')\n",
    "win = pg.GraphicsWindow(title=\"Testing GUI\")\n",
    "p1 = win.addPlot(title = \"TI's Algorithim\", row=1,col=0)\n",
    "p2 = win.addPlot(title = \"Project 16\", row=1,col=1)\n",
    "p1.setXRange(-6, 6)\n",
    "p1.setYRange(0, 6)\n",
    "p1.setLabel('left', text='Y position (m)')\n",
    "p1.setLabel('bottom', text='X position (m)')\n",
    "p2.setXRange(-6, 6)\n",
    "p2.setYRange(0, 6)\n",
    "p2.setLabel('left', text='Y position (m)')\n",
    "p2.setLabel('bottom', text='X position (m)')\n",
    "s1 = p1.plot([], [], pen=None, symbol='o')\n",
    "s2 = p2.plot([], [], pen=None, symbolBrush = (119,0,255), symbol='s', symbolSize=20)\n",
    "occupancyEstimate = win.addLabel(\"Occupancy  Estimate:0\",row=0,col=1, size='20pt', bold=True, color='FF0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsingMatFile = 'C:\\\\Users\\\\Abin\\\\Documents\\\\Github\\\\OccupancyDetection\\\\Data\\\\Experiment 3\\\\2PeopleWalkingRahulThanu.mat'\n",
    "tlvData = (loadmat(parsingMatFile))['tlvStream'][0]\n",
    "\n",
    "#Initialise Kalman Parameters \n",
    "centroidX =np.zeros((4,1)) #Centroid X contains all tracked centroids/occupant\n",
    "centroidP = [] #Centroid P contains 4x4 error covariance matrix of each occupant/centroid\n",
    "P = np.identity(4) #initialise first occupant\n",
    "centroidP.extend([P])\n",
    "Q = np.multiply(100,np.identity(4)) #system covariance matrix\n",
    "R = np.multiply(0.01,np.array([[1],[1]])) #measurement covariance matrix\n",
    "#tree based\n",
    "weightThresholdIntial = 0.2 #minimum distance between points\n",
    "minClusterSizeInitial = 10\n",
    "weightThresholdFinal = 0.8 #minimum distance between points\n",
    "minClusterSizeFinal = 8 \n",
    "\n",
    "#zone snr\n",
    "snrFirstZone = 5\n",
    "snrMiddleZone = 15\n",
    "snrLastZone = 10\n",
    "\n",
    "tlvHeaderLengthInBytes = 8\n",
    "pointLengthInBytes = 16\n",
    "targetLengthInBytes = 68\n",
    "\n",
    "tiPosX = np.array([])\n",
    "tiPosY = np.array([])\n",
    "\n",
    "tiOutput = np.array([])\n",
    "kalmanOutput = np.array([])\n",
    "clusteringOutput = np.array([])\n",
    "snrFilteredClusters = np.array([])\n",
    "groundTruth = 0\n",
    "xPositions = np.array([])\n",
    "\n",
    "groundTruthDataCollection = False\n",
    "locationDataNeeded = False\n",
    "xGroup16 = np.array([])\n",
    "yGroup16 = np.array([])\n",
    "xLocationTI = np.array([])\n",
    "yLocationTI = np.array([])\n",
    "\n",
    "isFirst = 1\n",
    "\n",
    "for index in range(0, len(tlvData)):\n",
    "    tlvStream = tlvData[index]\n",
    "    #parsing\n",
    "    pointCloud, targetDict = tlvParsing(tlvStream, tlvHeaderLengthInBytes, pointLengthInBytes, targetLengthInBytes)\n",
    "\n",
    "    if pointCloud.size > 0:\n",
    "        posX,posY,SNR = parsePointCloud(pointCloud) #dictionary that contains the point cloud data\n",
    "        \n",
    "        #initial noise reduction\n",
    "        clusters = TreeClustering(posX, posY, SNR,weightThresholdIntial, minClusterSizeInitial)\n",
    "\n",
    "        if clusters.size > 0:\n",
    "#             row 1 - x\n",
    "#             row 2 - y\n",
    "#             row 3 - SNR\n",
    "#             row 4 - cluster number\n",
    "\n",
    "#             snr zone snr test\n",
    "#             4.5 to the end -> last zone\n",
    "#             3-4.5m -> middle zone\n",
    "#             1-3m -> first zone\n",
    "            snrMask_LastZone = np.logical_and(np.greater(clusters[1,:], 4.5), np.greater(clusters[2,:], snrLastZone)) #zone 4.5m and greater\n",
    "            snrMask_MiddleZone = np.logical_and(np.logical_and(np.greater(clusters[1,:], 3), np.less_equal(clusters[1,:], 4.5)), \n",
    "                                                np.greater(clusters[2,:], snrMiddleZone)) #zone 3-4.5m with SNR > 20\n",
    "            snrMask_FirstZone = np.logical_and(np.less_equal(clusters[1,:], 3), np.greater(clusters[2,:], snrFirstZone))\n",
    "            overallSnrMask = np.logical_or(np.logical_or(snrMask_FirstZone,snrMask_MiddleZone), snrMask_LastZone)\n",
    "\n",
    "            snrFilteredClusters = clusters[:,overallSnrMask]\n",
    "            \n",
    "            if snrFilteredClusters.size > 0:\n",
    "                dbClusters = TreeClustering(snrFilteredClusters[0,:], snrFilteredClusters[1,:], \n",
    "                                                snrFilteredClusters[2,:], \n",
    "                                                weightThresholdFinal, minClusterSizeFinal)\n",
    "                if dbClusters.size > 0:\n",
    "                    #row 1 - x\n",
    "                    #row 2 - y\n",
    "                    #row 3 - cluster number\n",
    "                    k = int(max(dbClusters[3,:])) + 1 \n",
    "                    points = np.transpose(np.array([dbClusters[0,:], dbClusters[1,:]]))\n",
    "\n",
    "                    #kmeans \n",
    "                    centroidClusterer = KMeans(n_clusters= k).fit(points)\n",
    "                    centroidData = np.array([centroidClusterer.cluster_centers_[:,0], centroidClusterer.cluster_centers_[:,1]])\n",
    "\n",
    "                    #tracking\n",
    "                    centroidX, centroidP,isFirst = LiveRKF(centroidData, centroidX, centroidP, Q, R, isFirst)\n",
    "                    #calculate x and y positions\n",
    "                    xPositions = np.multiply(centroidX[0,:], np.cos(centroidX[2,:]))\n",
    "                    yPositions = np.multiply(centroidX[0,:], np.sin(centroidX[2,:]))\n",
    "                    mask = np.logical_and((np.logical_and(xPositions<3,xPositions>-4)), np.logical_and(yPositions<5.5,yPositions>0.5))\n",
    "                    xPositions = xPositions[mask]\n",
    "                    yPositions= yPositions[mask]\n",
    "                    #keep centroids that are inside the constraints \n",
    "                    centroidX = centroidX[:,mask]\n",
    "                    \n",
    "    if len(targetDict) != 0:\n",
    "        #kinematic data object structure\n",
    "        #row 0 - posX\n",
    "        #row 1 - posY \n",
    "        #row 2 - velX\n",
    "        #row 3 - velY\n",
    "        #row 4 - accX\n",
    "        #row 5 - accY\n",
    "        tiPosX = targetDict['kinematicData'][0,:]\n",
    "        tiPosY = targetDict['kinematicData'][1,:]\n",
    "        #enforce limits on TI \n",
    "        mask = np.logical_and(np.logical_and(tiPosX<3,tiPosX>-4),np.logical_and(tiPosY<5.5,tiPosY>0.5))\n",
    "        xTi = tiPosX[mask]\n",
    "        yTi = tiPosY[mask]\n",
    "    \n",
    "    if groundTruthDataCollection:\n",
    "        if clusters.size > 0:\n",
    "            #enforce noise reduced point cloud v shape   \n",
    "            xClusters = clusters[0,:]\n",
    "            yClusters = clusters[1,:]\n",
    "            mask = np.logical_and((np.logical_and(xClusters<3,xClusters>-4)), np.logical_and(yClusters<5.5,yClusters>0.5))\n",
    "            xClusters = xClusters[mask]\n",
    "            yClusters = yClusters[mask]\n",
    "        #no one in ground truth\n",
    "        if clusters.size > 0:\n",
    "            s1.setData(xClusters, yClusters)\n",
    "        else:\n",
    "            print('AUTOFILL')\n",
    "            s1.setData([],[])\n",
    "            s2.setData([],[])\n",
    "            QtGui.QApplication.processEvents()\n",
    "            kalmanOutput = np.append(kalmanOutput, 111)\n",
    "            clusteringOutput = np.append(clusteringOutput, 111)\n",
    "            continue\n",
    "\n",
    "        if snrFilteredClusters.size > 0:\n",
    "            s2.setData(snrFilteredClusters[0,:],snrFilteredClusters[1,:])\n",
    "        else:\n",
    "            s2.setData([],[])\n",
    "\n",
    "        QtGui.QApplication.processEvents()\n",
    "\n",
    "    else:\n",
    "        #if doing TI vs Alg\n",
    "        \n",
    "        #plotting code for TI vs Group16\n",
    "        if len(targetDict) != 0:\n",
    "            s1.setData(xTi, yTi)\n",
    "            tiOutput = np.append(tiOutput, len(tiPosX))\n",
    "            xLocationTI = np.append(xLocationTI,xTi)\n",
    "            yLocationTI = np.append(yLocationTI, yTi)\n",
    "        else:\n",
    "            s1.setData([],[])\n",
    "            tiOutput = np.append(tiOutput, len(tiPosX))\n",
    "            xLocationTI = np.append(xLocationTI,np.NaN)\n",
    "            yLocationTI = np.append(yLocationTI, np.NaN)\n",
    "            \n",
    "        if xPositions.size > 0:\n",
    "            s2.setData(xPositions,yPositions)\n",
    "            xGroup16 = np.append(xGroup16, xPositions)\n",
    "            yGroup16 = np.append(yGroup16,yPositions)\n",
    "            kalmanOutput = np.append(kalmanOutput, len(xPositions))\n",
    "        else:\n",
    "            s2.setData([],[])\n",
    "            kalmanOutput = np.append(kalmanOutput, len(xPositions))\n",
    "            xGroup16 = np.append(xGroup16, np.NaN)\n",
    "            yGroup16 = np.append(yGroup16,np.NaN)\n",
    "            \n",
    "            \n",
    "        numberOfTargets = len(xPositions) #compute how many occupants\n",
    "        message = \"Occupancy Estimate: \" + str(numberOfTargets)\n",
    "        win.removeItem(occupancyEstimate)#Remove current message\n",
    "        occupancyEstimate = win.addLabel(message, row=0,col=1, size='20pt', bold=True, color='FF0000') #add new\n",
    "        QtGui.QApplication.processEvents()\n",
    "            \n",
    "        QtGui.QApplication.processEvents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write algVsTI to csv\n",
    "if locationDataNeeded:\n",
    "    labellingDf = pd.DataFrame(np.transpose(np.array([tiOutput, kalmanOutput,xLocationTI,yLocationTI,xGroup16,yGroup16])))\n",
    "    labellingDf.columns = ['TI', 'Group 16', 'xTi', 'yTi', 'xGroup16', 'yGroup16']\n",
    "    labellingDf.to_csv('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Results\\\\Algorithm vs TI\\\\LocationX3.csv')\n",
    "else:\n",
    "    labellingDf = pd.DataFrame(np.transpose(np.array([tiOutput, kalmanOutput])))\n",
    "    labellingDf.columns = ['TI', 'Group 16']\n",
    "    labellingDf.to_csv('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Results\\\\Algorithm vs TI\\\\2PeopleSitting - D3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
