{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import time\n",
    "from scipy.spatial.distance import euclidean\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import time\n",
    "import pyqtgraph as pg\n",
    "from pyqtgraph.Qt import QtCore, QtGui\n",
    "%gui qt5\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one person slow walk\n",
    "#TLV Data Load\n",
    "tlvData = (loadmat('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\slowWalk.mat'))['tlvStream'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two people slow walk\n",
    "#TLV Data Load\n",
    "#loads of points\n",
    "tlvData = (loadmat('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\2PeopleMoving.mat'))['tlvStream'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three people walk\n",
    "#TLV Data Load\n",
    "tlvData = (loadmat('C:\\\\Users\\\\hasna\\\\Documents\\\\GitHub\\\\OccupancyDetection\\\\Data\\\\Matlab Data\\\\3PeopleWalking.mat'))['tlvStream'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise variables\n",
    "lostSync = False\n",
    "\n",
    "#valid header variables and constant\n",
    "magicBytes = np.array([2,1,4,3,6,5,8,7], dtype= 'uint8')\n",
    "\n",
    "isMagicOk = False\n",
    "isDataOk = False\n",
    "gotHeader = False\n",
    "\n",
    "frameHeaderLength = 52 #52 bytes long\n",
    "tlvHeaderLengthInBytes = 8\n",
    "pointLengthInBytes = 16\n",
    "frameNumber = 1\n",
    "targetFrameNumber = 0\n",
    "targetLengthInBytes = 68\n",
    "\n",
    "#graph constraints\n",
    "weightThreshold = 0.5 #minimum distance between points\n",
    "minClusterSize = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = QtGui.QApplication([])\n",
    "pg.setConfigOption('background','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pg.GraphicsWindow(title=\"Testing GUI\")\n",
    "plot1 = win.addPlot()\n",
    "plot1.setXRange(-6,6)\n",
    "plot1.setYRange(0,6)\n",
    "plot1.setLabel('left',text = 'Y position (m)')\n",
    "plot1.setLabel('bottom', text= 'X position (m)')\n",
    "s1 = plot1.plot([],[],pen=None,symbol='o')\n",
    "\n",
    "plot2 = win.addPlot()\n",
    "plot2.setXRange(-6,6)\n",
    "plot2.setYRange(0,6)\n",
    "plot2.setLabel('left',text = 'Y position (m)')\n",
    "plot2.setLabel('bottom', text= 'X position (m)')\n",
    "s2 = plot2.plot([],[],pen=None,symbol='o')\n",
    "\n",
    "plot3 = win.addPlot()\n",
    "plot3.setXRange(-6,6)\n",
    "plot3.setYRange(0,6)\n",
    "plot3.setLabel('left',text = 'Y position (m)')\n",
    "plot3.setLabel('bottom', text= 'X position (m)')\n",
    "s3 = plot3.plot([],[],pen=None,symbol='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering only using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyqtgraph\\graphicsItems\\GraphicsWidget.py\u001b[0m in \u001b[0;36mboundingRect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mboundingRect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mbr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapRectFromParent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m#print \"bounds:\", br\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-819efd4c47b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m                             \u001b[0ms3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpointsX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointsY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                             \u001b[0mQtGui\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQApplication\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessEvents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for tlvStream in tlvData:\n",
    "    tlvStream = np.frombuffer(tlvStream, dtype = 'uint8')\n",
    "    \n",
    "    #tlv header\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = tlvStream[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = tlvStream[index+4:index+8].view(dtype=np.uint32)\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    tlvDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "\n",
    "    if tlvType == 6: \n",
    "        numberOfPoints = tlvDataLength/pointLengthInBytes\n",
    "        p = tlvStream[index:index+tlvDataLength[0]].view(np.single)\n",
    "        pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "\n",
    "        if not(pointCloud is None):\n",
    "            #constrain point cloud to within the effective sensor range\n",
    "            #range 1 < x < 6\n",
    "            #azimuth -50 deg to 50 deg\n",
    "            #check whether corresponding range and azimuth data are within the constraints\n",
    "\n",
    "            effectivePointCloud = np.array([])\n",
    "            for index in range(0, len(pointCloud[0,:])):\n",
    "                if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "                and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "                     and pointCloud[1, index] < 50*np.pi/180):\n",
    "        \n",
    "                    #concatenate columns to the new point cloud\n",
    "                    if len(effectivePointCloud) == 0:\n",
    "                        effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "                    else:\n",
    "                        point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                        effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "                        \n",
    "\n",
    "            if len(effectivePointCloud) != 0:\n",
    "                posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "                posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "                points = np.array([posX, posY])\n",
    "                s1.setData(posX, posY)\n",
    "                QtGui.QApplication.processEvents()\n",
    "                \n",
    "                #use DBSCAN to filter out \n",
    "                clusterer = DBSCAN(eps=0.4, min_samples=15)\n",
    "                clusterer.fit(pd.DataFrame(np.transpose(np.array([posX,posY]))).values)\n",
    "                \n",
    "                if clusterer.core_sample_indices_.size > 0:\n",
    "                    #array that contains the x,y positions and the cluster association number\n",
    "                    clusters = np.array([posX[clusterer.core_sample_indices_],\n",
    "                              posY[clusterer.core_sample_indices_], \n",
    "                             clusterer.labels_[clusterer.core_sample_indices_]])\n",
    "                    s2.setData(clusters[0,:], clusters[1,:])\n",
    "                    QtGui.QApplication.processEvents()\n",
    "#                     for centroidNumber in np.unique(clusters[2,:]):\n",
    "#                         xMean = np.mean(clusters[0,:][np.isin(clusters[2,:], centroidNumber)])\n",
    "#                         yMean = np.mean(clusters[1,:][np.isin(clusters[2,:], centroidNumber)])\n",
    "\n",
    "\n",
    "                points = np.array([posX, posY])\n",
    "                #posX and posY given by \n",
    "                vertexID = np.arange(len(posX))\n",
    "                vertexList = np.arange(len(posX))\n",
    "                \n",
    "                if len(posX) >= minClusterSize:\n",
    "                    edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "\n",
    "                    #create distance matrix\n",
    "                    #x1 - x0\n",
    "                    xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                                              np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "                    #y1 - y0\n",
    "                    yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                                              np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "                    #euclidean distance calculation\n",
    "                    edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "                    #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "                    weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "                    edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "                    #perform iterative dfs\n",
    "                    pointsX = np.array([])\n",
    "                    pointsY = np.array([])\n",
    "                    \n",
    "                    centroidNumber = 0\n",
    "                    while vertexID.size > 0:\n",
    "                        startNode = vertexID[0]\n",
    "                        visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "                        #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "                        vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "                        #visited is a component, extract cluster from it if possible\n",
    "                        if visited.size >= minClusterSize:\n",
    "                            pointsX = np.append(pointsX, posX[visited])\n",
    "                            pointsY = np.append(pointsY, posY[visited]) \n",
    "                            s3.setData(pointsX, pointsY)\n",
    "                            QtGui.QApplication.processEvents()\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0383162, 1.970533 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 27)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering and Noise Reduction using DBSCAN and Tree based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterativeDfs(vertexID, edgeMatrix, startNode):\n",
    "    \n",
    "    visited = np.array([], dtype=np.int)\n",
    "    dfsStack = np.array([startNode])\n",
    "\n",
    "    while np.logical_not(np.equal(dfsStack.size,0)):\n",
    "        vertex, dfsStack = dfsStack[-1], dfsStack[:-1] #equivalent to stack pop function\n",
    "        if vertex not in visited:\n",
    "            #find unvisited nodes\n",
    "            unvisitedNodes = vertexID[np.logical_not(np.isnan(edgeMatrix[int(vertex), :]))]\n",
    "            visited = np.append(visited, vertex)\n",
    "            #add unvisited nodes to the stack\n",
    "            dfsStack = np.append(dfsStack, unvisitedNodes[np.logical_not(np.isin(unvisitedNodes,visited))])\n",
    "    \n",
    "    return visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tlvStream in tlvData:\n",
    "    tlvStream = np.frombuffer(tlvStream, dtype = 'uint8')\n",
    "    \n",
    "    #tlv header\n",
    "    index = 0\n",
    "    #tlv header parsing\n",
    "    tlvType = tlvStream[index:index+4].view(dtype=np.uint32)\n",
    "    tlvLength = tlvStream[index+4:index+8].view(dtype=np.uint32)\n",
    "    \n",
    "    index += tlvHeaderLengthInBytes\n",
    "    tlvDataLength = tlvLength - tlvHeaderLengthInBytes\n",
    "\n",
    "    if tlvType == 6: \n",
    "        numberOfPoints = tlvDataLength/pointLengthInBytes\n",
    "        p = tlvStream[index:index+tlvDataLength[0]].view(np.single)\n",
    "        pointCloud = np.reshape(p,(4, int(numberOfPoints)),order=\"F\")\n",
    "\n",
    "        if not(pointCloud is None):\n",
    "            #constrain point cloud to within the effective sensor range\n",
    "            #range 1 < x < 6\n",
    "            #azimuth -50 deg to 50 deg\n",
    "            #check whether corresponding range and azimuth data are within the constraints\n",
    "\n",
    "            effectivePointCloud = np.array([])\n",
    "            for index in range(0, len(pointCloud[0,:])):\n",
    "                if (pointCloud[0,index] > 1 and pointCloud[0,index] < 6) \\\n",
    "                and (pointCloud[1, index] > -50*np.pi/180 \\\n",
    "                     and pointCloud[1, index] < 50*np.pi/180):\n",
    "        \n",
    "                    #concatenate columns to the new point cloud\n",
    "                    if len(effectivePointCloud) == 0:\n",
    "                        effectivePointCloud = np.reshape(pointCloud[:, index], (4,1), order=\"F\")\n",
    "                    else:\n",
    "                        point = np.reshape(pointCloud[:, index], (4,1),order=\"F\")\n",
    "                        effectivePointCloud = np.hstack((effectivePointCloud, point))\n",
    "                        \n",
    "\n",
    "            if len(effectivePointCloud) != 0:\n",
    "                posX = np.multiply(effectivePointCloud[0,:], np.sin(effectivePointCloud[1,:]))\n",
    "                posY = np.multiply(effectivePointCloud[0,:], np.cos(effectivePointCloud[1,:]))\n",
    "                points = np.array([posX, posY])\n",
    "                #posX and posY given by \n",
    "                vertexID = np.arange(len(posX))\n",
    "                vertexList = np.arange(len(posX))\n",
    "                clusterDf = pd.DataFrame([], columns=['X', 'Y', 'CentroidNumber'])\n",
    "                clusterDf.to_csv('CentroidData.csv', mode='a', header=True, index=False)\n",
    "                \n",
    "                if len(posX) >= minClusterSize:\n",
    "                    edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "\n",
    "                    #create distance matrix\n",
    "                    #x1 - x0\n",
    "                    xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                                              np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "                    #y1 - y0\n",
    "                    yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                                              np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "                    #euclidean distance calculation\n",
    "                    edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "\n",
    "                    #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "                    weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "                    edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "                    #perform iterative dfs\n",
    "                    pointsX = np.array([])\n",
    "                    pointsY = np.array([])\n",
    "                    \n",
    "                    centroidNumber = 0\n",
    "                    while vertexID.size > 0:\n",
    "                        startNode = vertexID[0]\n",
    "                        visited = iterativeDfs(vertexList, edgeMatrix, startNode)\n",
    "                        #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "                        vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "                        #visited is a component, extract cluster from it if possible\n",
    "                        if visited.size >= minClusterSize:\n",
    "                            pointsX = np.append(pointsX, posX[visited])\n",
    "                            pointsY = np.append(pointsY, posY[visited]) \n",
    "                    \n",
    "                    if pointsX.size == 0:\n",
    "                        centroidDf = pd.DataFrame(np.expand_dims(np.array([np.NaN, np.NaN, 0]), axis=0))\n",
    "                        centroidDf.to_csv('CentroidData.csv', mode='a', index=False, header=None)\n",
    "                    else:\n",
    "                        clusterer = DBSCAN(eps=0.5, min_samples=20)\n",
    "                        clusterer.fit(pd.DataFrame(np.transpose(np.array([pointsX,pointsY]))).values)\n",
    "\n",
    "                        if clusterer.core_sample_indices_.size > 0:\n",
    "                            #array that contains the x,y positions and the cluster association number\n",
    "                            clusters = np.array([pointsX[clusterer.core_sample_indices_],\n",
    "                                      pointsY[clusterer.core_sample_indices_], \n",
    "                                     clusterer.labels_[clusterer.core_sample_indices_]])\n",
    "                            for centroidNumber in np.unique(clusters[2,:]):\n",
    "                                xMean = np.mean(clusters[0,:][np.isin(clusters[2,:], centroidNumber)])\n",
    "                                yMean = np.mean(clusters[1,:][np.isin(clusters[2,:], centroidNumber)])\n",
    "                                centroidDf = pd.DataFrame(np.expand_dims(np.array([xMean, yMean, centroidNumber]), axis=0))\n",
    "                                centroidDf.to_csv('CentroidData.csv', mode='a', index=False, header=None)\n",
    "                        else:\n",
    "                            centroidDf = pd.DataFrame(np.expand_dims(np.array([np.NaN, np.NaN, 0]), axis=0))\n",
    "                            centroidDf.to_csv('CentroidData.csv', mode='a', index=False, header=None)\n",
    "                else:\n",
    "                    centroidDf = pd.DataFrame(np.expand_dims(np.array([np.NaN, np.NaN, 0]), axis=0))\n",
    "                    centroidDf.to_csv('CentroidData.csv', mode='a', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "timingDf = pd.DataFrame({'Recursive': recursiveTiming, 'Iterative': iterativeTiming})\n",
    "timingDf.to_csv('ClusteringAlgorithmTiming.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Time (s)')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot timing characteristics\n",
    "plt.figure()\n",
    "plt.plot(timingDf['Recursive'])\n",
    "plt.plot(timingDf['Iterative'])\n",
    "plt.title('Timing Characteristics')\n",
    "plt.xlabel('Instance')\n",
    "plt.ylabel('Time (s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfs unit test and overall algorithm test set \n",
    "posX = np.array([2,2.1,2.2,2.1,2.2,2.2,5,5.1,5,4,5])\n",
    "posY = np.array([4,4,4,3.8,3.9,3.95,2,2,2.1,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit test to check vertex and edge generation\n",
    "posX = np.array([1,1,1,2,3])\n",
    "posY = np.array([4,3,2,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "posX = np.array([1,2,3])\n",
    "posY = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redesign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posX and posY given by \n",
    "vertexID = np.arange(len(posX))\n",
    "\n",
    "if len(posX) >= minClusterSize:\n",
    "    edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "    \n",
    "    #create distance matrix\n",
    "    #x1 - x0\n",
    "    xDifference = np.subtract(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX)), \n",
    "                              np.transpose(np.repeat(posX, repeats=len(posX)).reshape(len(posX), len(posX))))\n",
    "    #y1 - y0\n",
    "    yDifference = np.subtract(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY)), \n",
    "                              np.transpose(np.repeat(posY, repeats=len(posY)).reshape(len(posY), len(posY))))\n",
    "    #euclidean distance calculation\n",
    "    edgeMatrix = np.sqrt(np.add(np.square(xDifference), np.square(yDifference)))\n",
    "    \n",
    "    #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "    weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "    edgeMatrix[weightMask] = np.NaN\n",
    "    \n",
    "    #perform iterative dfs\n",
    "    components = np.array([])\n",
    "    clusterDf = pd.DataFrame([], columns=['Frame'])\n",
    "    while vertexID.size > 0:\n",
    "        startNode = vertexID[0]\n",
    "        visited = iterativeDfs(vertexID, edgeMatrix, startNode)\n",
    "        #remove visited nodes (ie only slice off all unvisited nodes)\n",
    "        vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "        #visited is a component, extract cluster from it if possible\n",
    "        if visited.size >= minClusterSize:\n",
    "            cluster = np.array([posX[visited], posY[visited]])\n",
    "            centroid = np.array([np.mean(cluster[0,:]), np.mean(cluster[1,:])])\n",
    "            centroiDf = pd.DataFrame(np.expand_dims(centroid, axis=0), columns=['X', 'Y'])\n",
    "            centroiDf.to_csv('testCSV.csv', mode='a', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "visited = visited[:-1]\n",
    "print(visited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertexID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertexID = vertexID[np.logical_not(np.isin(vertexID, visited))]\n",
    "vertexID "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#vertex dataframe\n",
    "vertexID = np.arange(len(posX))\n",
    "vertexDf = pd.DataFrame({'VertexID':vertexID, 'X':posX, 'Y':posY})\n",
    "print()\n",
    "#minimum number of points to qualify as a person\n",
    "if len(vertexDf.values) >= minClusterSize: #do you even have one cluster\n",
    "    #preallocate edgeMatrix\n",
    "    #edges are denoted by their respective weights\n",
    "    #undirected graph with constraint that two nodes can only be connected by one edge\n",
    "    edgeMatrix = np.zeros((len(posX), len(posY)))\n",
    "    #vertices are saved as np arrays\n",
    "    #evaluate edge Matrix and create graph\n",
    "    for rowIndex in range(0, edgeMatrix.shape[0]):\n",
    "        for colIndex in range(0, edgeMatrix.shape[1]):\n",
    "            if rowIndex == colIndex:\n",
    "                continue #diagonal element\n",
    "            elif edgeMatrix[rowIndex, colIndex] != 0:\n",
    "                continue #element already filled\n",
    "            else:\n",
    "                pointA = (vertexDf.values[rowIndex])[1:] #extract x y position disregarding the vertexID\n",
    "                pointB = (vertexDf.values[colIndex])[1:] #extract x y position disregarding the vertexID\n",
    "                length = euclidean(pointA, pointB)\n",
    "                #fill elements\n",
    "                edgeMatrix[rowIndex,colIndex] = length\n",
    "                edgeMatrix[colIndex, rowIndex] = length\n",
    "\n",
    "    #weight based reduction of graph/remove edges by replacing edge weight by np.NaN\n",
    "    weightMask = np.logical_or(np.greater(edgeMatrix,weightThreshold), np.equal(edgeMatrix, 0))\n",
    "    edgeMatrix[weightMask] = np.NaN\n",
    "\n",
    "    #perform DFS to find connected components\n",
    "    componentsList = list() #list of components\n",
    "    vertexList = list() #used to hold vertices that have been considered\n",
    "    for vertex in vertexDf['VertexID']:\n",
    "        if vertex not in vertexList:\n",
    "            visitedNodes = recursiveDfs(vertexDf, edgeMatrix, vertex, visited=None)\n",
    "            componentsList.append(list(visitedNodes))\n",
    "            for vertex in visitedNodes:\n",
    "                if vertex not in vertexList:\n",
    "                    vertexList.append(vertex)\n",
    "                    \n",
    "    #use minimum cluster size to remove bad cluster matches\n",
    "    #and extract clusters\n",
    "    clusterList = list()\n",
    "    for cluster in componentsList:\n",
    "        if len(cluster) >= minClusterSize:\n",
    "            pointCluster = vertexDf.loc[np.array(cluster)]\n",
    "            clusterList.append(pointCluster)\n",
    "            \n",
    "else:\n",
    "    print('NOT ENOUGH POINTS')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
